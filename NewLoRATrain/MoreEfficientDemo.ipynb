{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "528ab85e",
   "metadata": {},
   "source": [
    "# üîß IMPORTANT: Kernel Selection\n",
    "\n",
    "**Before running any cells, make sure you select the correct Python kernel:**\n",
    "\n",
    "1. Click the **\"Select Kernel\"** button in the top-right corner of the notebook\n",
    "2. Choose **\"Python (envfin-416Final)\"** from the list\n",
    "   - This is your virtual environment with all dependencies installed\n",
    "3. If you don't see this option, restart VS Code and try again\n",
    "\n",
    "**Why this matters**: The notebook was crashing because it was using the wrong Python environment (`/opt/miniforge3/bin/python`) which doesn't have the required packages. Your correct environment is at `~/416Final/envfin/bin/python`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bad6d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16837d9",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Environment Setup\n",
    "\n",
    "**Training Mode:**\n",
    "- ‚úÖ **Standard LoRA** (no quantization)\n",
    "- Memory: ~15-20GB VRAM for Phi-3-mini\n",
    "- Method: Full precision + LoRA adapters\n",
    "\n",
    "**Memory Requirements:**\n",
    "- Phi-3-mini with LoRA typically requires ~15-20GB VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b9ca095d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CUDA available: True\n",
      "GPU: Quadro RTX 6000\n",
      "GPU Memory: 25.19 GB\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Use Phi-3-mini-128k for longer context (recommended)\n",
    "model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02450647",
   "metadata": {},
   "source": [
    "# LoRA Fine-tuning: Phi-3-mini for CAD-to-Language Generation\n",
    "\n",
    "This notebook demonstrates fine-tuning Phi-3-mini-128k using LoRA (Low-Rank Adaptation) to create a CAD-to-Language model using the CADmium dataset.\n",
    "\n",
    "## Architecture\n",
    "- **Base Model**: microsoft/Phi-3-mini-128k-instruct (longer context for CAD designs)\n",
    "- **Training Method**: LoRA (Low-Rank Adaptation)\n",
    "- **Dataset**: chandar-lab/CADmium (subset for demo)\n",
    "- **Task**: Natural Language ‚Üí CAD JSON generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1adae7",
   "metadata": {},
   "source": [
    "## 1. Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa3b9c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "# !pip install -q transformers datasets peft accelerate trl sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0571a1",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare CADmium Dataset\n",
    "\n",
    "We'll load a subset of the CADmium dataset and prepare it for fine-tuning with proper formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e42bf1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CADmium dataset...\n",
      "‚úÖ Loaded 1500 samples from CADmium\n",
      "Dataset columns: ['uid', 'annotation', 'json_desc']\n",
      "\n",
      "First example:\n",
      "{'uid': '0072/00726842', 'annotation': 'Begin by creating a rectangular prism with overall dimensions 0.75 long, 0.375 wide, and 0.46875 high. \\n\\nNext, modify the ends of the prism as follows:\\n\\nAt each of the four vertical corners (both at x=0 and x=0.75 along the length), replace the sharp edge with a quarter-circle arc of radius 0.1875, centered horizontally and vertically on the face. The top and bottom vertical edges on the short faces (width sides) are thus rounded, blending tangent to both edge and face.\\n\\nOn the top and bottom faces, ensure each end describes a smooth semicircular extension: extrude the width at both ends (x=0 and x=0.75) into a half-cylinder, each with a radius 0.1875 and center at (0.1875, 0.1875) for x=0 and (0.5625, 0.1875) for x=0.75. The total length from tip to tip, including both half-cylindrical ends, is 0.75.\\n\\nBlend all transitions smoothly so the entire object forms a rectangular solid with full width, but with its ends fully rounded in both plan and elevation, producing a pill-shaped bar with rectangular cross-section and semicircular ends on both main faces and edges. All features should be symmetrical about the object‚Äôs longitudinal axis.', 'json_desc': '{\"parts\": {\"part_1\": {\"coordinate_system\": {\"Euler Angles\": [0.0, 0.0, 0.0], \"Translation Vector\": [0.0, 0.0, 0.0]}, \"sketch\": {\"face_1\": {\"loop_1\": {\"arc_1\": {\"Start Point\": [0.0, 0.1875], \"Mid Point\": [0.0549, 0.3201], \"End Point\": [0.1875, 0.375]}, \"line_1\": {\"Start Point\": [0.1875, 0.375], \"End Point\": [0.0, 0.375]}, \"line_2\": {\"Start Point\": [0.0, 0.375], \"End Point\": [0.0, 0.1875]}}}, \"face_2\": {\"loop_1\": {\"line_1\": {\"Start Point\": [0.1875, 0.0], \"End Point\": [0.375, 0.0]}, \"line_2\": {\"Start Point\": [0.375, 0.0], \"End Point\": [0.375, 0.1875]}, \"arc_1\": {\"Start Point\": [0.375, 0.1875], \"Mid Point\": [0.3201, 0.0549], \"End Point\": [0.1875, 0.0]}}}, \"face_3\": {\"loop_1\": {\"arc_1\": {\"Start Point\": [0.375, 0.1875], \"Mid Point\": [0.4299, 0.3201], \"End Point\": [0.5625, 0.375]}, \"line_1\": {\"Start Point\": [0.5625, 0.375], \"End Point\": [0.375, 0.375]}, \"line_2\": {\"Start Point\": [0.375, 0.375], \"End Point\": [0.375, 0.1875]}}}, \"face_4\": {\"loop_1\": {\"line_1\": {\"Start Point\": [0.5625, 0.0], \"End Point\": [0.75, 0.0]}, \"line_2\": {\"Start Point\": [0.75, 0.0], \"End Point\": [0.75, 0.1875]}, \"arc_1\": {\"Start Point\": [0.75, 0.1875], \"Mid Point\": [0.6951, 0.0549], \"End Point\": [0.5625, 0.0]}}}}, \"extrusion\": {\"extrude_depth_towards_normal\": 0.4687, \"extrude_depth_opposite_normal\": 0.0, \"sketch_scale\": 0.75, \"operation\": \"NewBodyFeatureOperation\"}, \"description\": {\"name\": \"\", \"shape\": \"\", \"length\": 0.75, \"width\": 0.37499999999999994, \"height\": 0.46874999999999994}}, \"part_2\": {\"coordinate_system\": {\"Euler Angles\": [0.0, 0.0, 0.0], \"Translation Vector\": [0.0, 0.0, 0.0]}, \"sketch\": {\"face_1\": {\"loop_1\": {\"line_1\": {\"Start Point\": [0.0, 0.0], \"End Point\": [0.1875, 0.0]}, \"arc_1\": {\"Start Point\": [0.1875, 0.0], \"Mid Point\": [0.0549, 0.0549], \"End Point\": [0.0, 0.1875]}, \"line_2\": {\"Start Point\": [0.0, 0.1875], \"End Point\": [0.0, 0.0]}}}, \"face_2\": {\"loop_1\": {\"arc_1\": {\"Start Point\": [0.1875, 0.375], \"Mid Point\": [0.3201, 0.3201], \"End Point\": [0.375, 0.1875]}, \"line_1\": {\"Start Point\": [0.375, 0.1875], \"End Point\": [0.375, 0.375]}, \"line_2\": {\"Start Point\": [0.375, 0.375], \"End Point\": [0.1875, 0.375]}}}, \"face_3\": {\"loop_1\": {\"line_1\": {\"Start Point\": [0.375, 0.0], \"End Point\": [0.5625, 0.0]}, \"arc_1\": {\"Start Point\": [0.5625, 0.0], \"Mid Point\": [0.4299, 0.0549], \"End Point\": [0.375, 0.1875]}, \"line_2\": {\"Start Point\": [0.375, 0.1875], \"End Point\": [0.375, 0.0]}}}, \"face_4\": {\"loop_1\": {\"arc_1\": {\"Start Point\": [0.5625, 0.375], \"Mid Point\": [0.6951, 0.3201], \"End Point\": [0.75, 0.1875]}, \"line_1\": {\"Start Point\": [0.75, 0.1875], \"End Point\": [0.75, 0.375]}, \"line_2\": {\"Start Point\": [0.75, 0.375], \"End Point\": [0.5625, 0.375]}}}}, \"extrusion\": {\"extrude_depth_towards_normal\": 0.4687, \"extrude_depth_opposite_normal\": 0.0, \"sketch_scale\": 0.75, \"operation\": \"NewBodyFeatureOperation\"}, \"description\": {\"name\": \"\", \"shape\": \"\", \"length\": 0.75, \"width\": 0.37499999999999994, \"height\": 0.46874999999999994}}, \"part_3\": {\"coordinate_system\": {\"Euler Angles\": [0.0, 0.0, 0.0], \"Translation Vector\": [0.0, 0.0, 0.0]}, \"sketch\": {\"face_1\": {\"loop_1\": {\"circle_1\": {\"Center\": [0.1875, 0.1875], \"Radius\": 0.1875}}}, \"face_2\": {\"loop_1\": {\"circle_1\": {\"Center\": [0.5625, 0.1875], \"Radius\": 0.1875}}}}, \"extrusion\": {\"extrude_depth_towards_normal\": 0.4687, \"extrude_depth_opposite_normal\": 0.0, \"sketch_scale\": 0.75, \"operation\": \"NewBodyFeatureOperation\"}, \"description\": {\"name\": \"\", \"shape\": \"\", \"length\": 0.7499999999999999, \"width\": 0.37499999999999994, \"height\": 0.46874999999999994}}}}'}\n",
      "‚úÖ Loaded 1500 samples from CADmium\n",
      "Dataset columns: ['uid', 'annotation', 'json_desc']\n",
      "\n",
      "First example:\n",
      "{'uid': '0072/00726842', 'annotation': 'Begin by creating a rectangular prism with overall dimensions 0.75 long, 0.375 wide, and 0.46875 high. \\n\\nNext, modify the ends of the prism as follows:\\n\\nAt each of the four vertical corners (both at x=0 and x=0.75 along the length), replace the sharp edge with a quarter-circle arc of radius 0.1875, centered horizontally and vertically on the face. The top and bottom vertical edges on the short faces (width sides) are thus rounded, blending tangent to both edge and face.\\n\\nOn the top and bottom faces, ensure each end describes a smooth semicircular extension: extrude the width at both ends (x=0 and x=0.75) into a half-cylinder, each with a radius 0.1875 and center at (0.1875, 0.1875) for x=0 and (0.5625, 0.1875) for x=0.75. The total length from tip to tip, including both half-cylindrical ends, is 0.75.\\n\\nBlend all transitions smoothly so the entire object forms a rectangular solid with full width, but with its ends fully rounded in both plan and elevation, producing a pill-shaped bar with rectangular cross-section and semicircular ends on both main faces and edges. All features should be symmetrical about the object‚Äôs longitudinal axis.', 'json_desc': '{\"parts\": {\"part_1\": {\"coordinate_system\": {\"Euler Angles\": [0.0, 0.0, 0.0], \"Translation Vector\": [0.0, 0.0, 0.0]}, \"sketch\": {\"face_1\": {\"loop_1\": {\"arc_1\": {\"Start Point\": [0.0, 0.1875], \"Mid Point\": [0.0549, 0.3201], \"End Point\": [0.1875, 0.375]}, \"line_1\": {\"Start Point\": [0.1875, 0.375], \"End Point\": [0.0, 0.375]}, \"line_2\": {\"Start Point\": [0.0, 0.375], \"End Point\": [0.0, 0.1875]}}}, \"face_2\": {\"loop_1\": {\"line_1\": {\"Start Point\": [0.1875, 0.0], \"End Point\": [0.375, 0.0]}, \"line_2\": {\"Start Point\": [0.375, 0.0], \"End Point\": [0.375, 0.1875]}, \"arc_1\": {\"Start Point\": [0.375, 0.1875], \"Mid Point\": [0.3201, 0.0549], \"End Point\": [0.1875, 0.0]}}}, \"face_3\": {\"loop_1\": {\"arc_1\": {\"Start Point\": [0.375, 0.1875], \"Mid Point\": [0.4299, 0.3201], \"End Point\": [0.5625, 0.375]}, \"line_1\": {\"Start Point\": [0.5625, 0.375], \"End Point\": [0.375, 0.375]}, \"line_2\": {\"Start Point\": [0.375, 0.375], \"End Point\": [0.375, 0.1875]}}}, \"face_4\": {\"loop_1\": {\"line_1\": {\"Start Point\": [0.5625, 0.0], \"End Point\": [0.75, 0.0]}, \"line_2\": {\"Start Point\": [0.75, 0.0], \"End Point\": [0.75, 0.1875]}, \"arc_1\": {\"Start Point\": [0.75, 0.1875], \"Mid Point\": [0.6951, 0.0549], \"End Point\": [0.5625, 0.0]}}}}, \"extrusion\": {\"extrude_depth_towards_normal\": 0.4687, \"extrude_depth_opposite_normal\": 0.0, \"sketch_scale\": 0.75, \"operation\": \"NewBodyFeatureOperation\"}, \"description\": {\"name\": \"\", \"shape\": \"\", \"length\": 0.75, \"width\": 0.37499999999999994, \"height\": 0.46874999999999994}}, \"part_2\": {\"coordinate_system\": {\"Euler Angles\": [0.0, 0.0, 0.0], \"Translation Vector\": [0.0, 0.0, 0.0]}, \"sketch\": {\"face_1\": {\"loop_1\": {\"line_1\": {\"Start Point\": [0.0, 0.0], \"End Point\": [0.1875, 0.0]}, \"arc_1\": {\"Start Point\": [0.1875, 0.0], \"Mid Point\": [0.0549, 0.0549], \"End Point\": [0.0, 0.1875]}, \"line_2\": {\"Start Point\": [0.0, 0.1875], \"End Point\": [0.0, 0.0]}}}, \"face_2\": {\"loop_1\": {\"arc_1\": {\"Start Point\": [0.1875, 0.375], \"Mid Point\": [0.3201, 0.3201], \"End Point\": [0.375, 0.1875]}, \"line_1\": {\"Start Point\": [0.375, 0.1875], \"End Point\": [0.375, 0.375]}, \"line_2\": {\"Start Point\": [0.375, 0.375], \"End Point\": [0.1875, 0.375]}}}, \"face_3\": {\"loop_1\": {\"line_1\": {\"Start Point\": [0.375, 0.0], \"End Point\": [0.5625, 0.0]}, \"arc_1\": {\"Start Point\": [0.5625, 0.0], \"Mid Point\": [0.4299, 0.0549], \"End Point\": [0.375, 0.1875]}, \"line_2\": {\"Start Point\": [0.375, 0.1875], \"End Point\": [0.375, 0.0]}}}, \"face_4\": {\"loop_1\": {\"arc_1\": {\"Start Point\": [0.5625, 0.375], \"Mid Point\": [0.6951, 0.3201], \"End Point\": [0.75, 0.1875]}, \"line_1\": {\"Start Point\": [0.75, 0.1875], \"End Point\": [0.75, 0.375]}, \"line_2\": {\"Start Point\": [0.75, 0.375], \"End Point\": [0.5625, 0.375]}}}}, \"extrusion\": {\"extrude_depth_towards_normal\": 0.4687, \"extrude_depth_opposite_normal\": 0.0, \"sketch_scale\": 0.75, \"operation\": \"NewBodyFeatureOperation\"}, \"description\": {\"name\": \"\", \"shape\": \"\", \"length\": 0.75, \"width\": 0.37499999999999994, \"height\": 0.46874999999999994}}, \"part_3\": {\"coordinate_system\": {\"Euler Angles\": [0.0, 0.0, 0.0], \"Translation Vector\": [0.0, 0.0, 0.0]}, \"sketch\": {\"face_1\": {\"loop_1\": {\"circle_1\": {\"Center\": [0.1875, 0.1875], \"Radius\": 0.1875}}}, \"face_2\": {\"loop_1\": {\"circle_1\": {\"Center\": [0.5625, 0.1875], \"Radius\": 0.1875}}}}, \"extrusion\": {\"extrude_depth_towards_normal\": 0.4687, \"extrude_depth_opposite_normal\": 0.0, \"sketch_scale\": 0.75, \"operation\": \"NewBodyFeatureOperation\"}, \"description\": {\"name\": \"\", \"shape\": \"\", \"length\": 0.7499999999999999, \"width\": 0.37499999999999994, \"height\": 0.46874999999999994}}}}'}\n"
     ]
    }
   ],
   "source": [
    "# Load CADmium dataset (subset for demo to save memory)\n",
    "print(\"Loading CADmium dataset...\")\n",
    "try:\n",
    "    # Load from HuggingFace - we'll use a manageable subset\n",
    "    dataset = load_dataset(\"chandar-lab/CADmium-ds\", split=\"train\", streaming=False)\n",
    "\n",
    "    # Take a subset sized for the extraction LoRA run\n",
    "    num_samples = 1500\n",
    "    dataset = dataset.shuffle(seed=42).select(range(min(num_samples, len(dataset))))\n",
    "\n",
    "    print(f\"‚úÖ Loaded {len(dataset)} samples from CADmium\")\n",
    "    print(f\"Dataset columns: {dataset.column_names}\")\n",
    "    print(f\"\\nFirst example:\")\n",
    "    print(dataset[0])\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading dataset: {e}\")\n",
    "    print(\"\\nUsing synthetic examples for demonstration...\")\n",
    "    synthetic_data = [\n",
    "        {\n",
    "            \"name\": f\"synthetic_{i}\",\n",
    "            \"annotation\": \"Create a rectangular block 10mm x 5mm x 3mm at the origin\",\n",
    "            \"sequence\": {\n",
    "                \"parts\": {\n",
    "                    \"part_0\": {\n",
    "                        \"sketch\": {\n",
    "                            \"type\": \"rectangle\",\n",
    "                            \"center\": [0, 0, 0],\n",
    "                            \"width\": 0.01,\n",
    "                            \"height\": 0.005\n",
    "                        },\n",
    "                        \"frame\": \"world\"\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        for i in range(20)\n",
    "    ]\n",
    "    dataset = Dataset.from_list(synthetic_data)\n",
    "    print(f\"‚úÖ Created synthetic dataset with {len(dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d353d777",
   "metadata": {},
   "source": [
    "### Define Helper Functions for Parameter Extraction\n",
    "\n",
    "These functions will be used throughout the notebook to flatten/unflatten CAD parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8f3db18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Helper functions defined:\n",
      "   - _normalize_sequence()\n",
      "   - _flatten()\n",
      "   - extract_all_parameters_from_sequence()\n",
      "   - _parse_path()\n",
      "   - unflatten_parameters()\n"
     ]
    }
   ],
   "source": [
    "# Helper functions for CAD parameter extraction\n",
    "from collections import OrderedDict\n",
    "\n",
    "def _normalize_sequence(raw_sequence):\n",
    "    \"\"\"Ensure the CAD sequence is a Python dict.\"\"\"\n",
    "    if isinstance(raw_sequence, dict):\n",
    "        return raw_sequence\n",
    "    if isinstance(raw_sequence, str):\n",
    "        try:\n",
    "            return json.loads(raw_sequence)\n",
    "        except json.JSONDecodeError:\n",
    "            return {\"raw\": raw_sequence}\n",
    "    return {}\n",
    "\n",
    "def _flatten(obj, prefix=\"\", store=None):\n",
    "    \"\"\"Recursively flatten nested CAD structures using dot + index notation.\"\"\"\n",
    "    if store is None:\n",
    "        store = OrderedDict()\n",
    "\n",
    "    if isinstance(obj, dict):\n",
    "        for key, value in obj.items():\n",
    "            new_prefix = f\"{prefix}.{key}\" if prefix else key\n",
    "            _flatten(value, new_prefix, store)\n",
    "    elif isinstance(obj, list):\n",
    "        for idx, value in enumerate(obj):\n",
    "            new_prefix = f\"{prefix}[{idx}]\" if prefix else f\"[{idx}]\"\n",
    "            _flatten(value, new_prefix, store)\n",
    "    else:\n",
    "        store[prefix] = obj\n",
    "    return store\n",
    "\n",
    "def extract_all_parameters_from_sequence(raw_sequence):\n",
    "    \"\"\"Return an ordered mapping of every parameter path ‚Üí value contained in CADmium.\"\"\"\n",
    "    normalized = _normalize_sequence(raw_sequence)\n",
    "    flattened = _flatten(normalized)\n",
    "    return {path: flattened[path] for path in flattened}\n",
    "\n",
    "def _parse_path(path):\n",
    "    \"\"\"Split a flattened key into list/dict navigation tokens.\"\"\"\n",
    "    tokens = []\n",
    "    i = 0\n",
    "    while i < len(path):\n",
    "        if path[i] == '[':\n",
    "            j = path.find(']', i)\n",
    "            tokens.append(int(path[i + 1:j]))\n",
    "            i = j + 1\n",
    "            if i < len(path) and path[i] == '.':\n",
    "                i += 1\n",
    "        else:\n",
    "            j = i\n",
    "            while j < len(path) and path[j] not in '.[':\n",
    "                j += 1\n",
    "            tokens.append(path[i:j])\n",
    "            i = j\n",
    "            if i < len(path) and path[i] == '.':\n",
    "                i += 1\n",
    "    return [token for token in tokens if token != \"\"]\n",
    "\n",
    "def unflatten_parameters(flat_params):\n",
    "    \"\"\"Reconstruct the original nested structure from flattened parameter paths.\"\"\"\n",
    "    root = None\n",
    "    for path, value in flat_params.items():\n",
    "        tokens = _parse_path(path)\n",
    "        if not tokens:\n",
    "            root = value\n",
    "            continue\n",
    "        if root is None:\n",
    "            root = [] if isinstance(tokens[0], int) else {}\n",
    "        current = root\n",
    "        for idx, token in enumerate(tokens):\n",
    "            is_last = idx == len(tokens) - 1\n",
    "            next_token = tokens[idx + 1] if not is_last else None\n",
    "\n",
    "            if isinstance(token, str):\n",
    "                if not isinstance(current, dict):\n",
    "                    raise TypeError(f\"Expected dict while rebuilding path '{path}', found {type(current)}\")\n",
    "                if is_last:\n",
    "                    current[token] = value\n",
    "                else:\n",
    "                    if token not in current or current[token] is None:\n",
    "                        current[token] = [] if isinstance(next_token, int) else {}\n",
    "                    current = current[token]\n",
    "            else:\n",
    "                if not isinstance(current, list):\n",
    "                    raise TypeError(f\"Expected list while rebuilding path '{path}', found {type(current)}\")\n",
    "                while len(current) <= token:\n",
    "                    current.append(None)\n",
    "                if is_last:\n",
    "                    current[token] = value\n",
    "                else:\n",
    "                    if current[token] is None:\n",
    "                        current[token] = [] if isinstance(next_token, int) else {}\n",
    "                    current = current[token]\n",
    "    return {} if root is None else root\n",
    "\n",
    "print(\"‚úÖ Helper functions defined:\")\n",
    "print(\"   - _normalize_sequence()\")\n",
    "print(\"   - _flatten()\")\n",
    "print(\"   - extract_all_parameters_from_sequence()\")\n",
    "print(\"   - _parse_path()\")\n",
    "print(\"   - unflatten_parameters()\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cddae70",
   "metadata": {},
   "source": [
    "### Build Extraction Training Dataset\n",
    "\n",
    "Create training pairs: Natural language annotation ‚Üí Predicted CAD parameters (JSON only, no metadata)\n",
    "\n",
    "**Training Strategy:**\n",
    "- Model learns to **predict** parameter values from instructions\n",
    "- Padding with 0 teaches the model which parameters are relevant vs. irrelevant\n",
    "- Model can generalize to diverse situations by inferring reasonable defaults\n",
    "- Missing information in instructions ‚Üí model learns appropriate 0/default patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "136976bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building parameter prediction training dataset...\n",
      "üìå Using 'json_desc' field (not 'sequence')\n",
      "üìå OUTPUT: Predicted parameter values as JSON (NO name, NO annotation)\n",
      "üìå STRATEGY: Model learns which parameters are relevant (non-zero) vs irrelevant (0)\n",
      "================================================================================\n",
      "  Collecting all parameter paths for padding...\n",
      "  Found 2286 unique parameter paths\n",
      "  Processed 300/1500 examples...\n",
      "  Processed 300/1500 examples...\n",
      "  Processed 600/1500 examples...\n",
      "  Processed 600/1500 examples...\n",
      "  Processed 900/1500 examples...\n",
      "  Processed 900/1500 examples...\n",
      "  Processed 1200/1500 examples...\n",
      "  Processed 1200/1500 examples...\n",
      "  Processed 1500/1500 examples...\n",
      "  Processed 1500/1500 examples...\n",
      "================================================================================\n",
      "‚úÖ Extraction dataset created: 1500 samples\n",
      "\n",
      "üìä Example statistics:\n",
      "   Total parameter count: 2286\n",
      "   Non-zero parameters: 118\n",
      "   Padding ratio: 94.8%\n",
      "\n",
      "   First assistant response preview (first 300 chars):\n",
      "{\n",
      "  \"parts.part_1.coordinate_system.Euler Angles[0]\": 0.0,\n",
      "  \"parts.part_1.coordinate_system.Euler Angles[1]\": 0.0,\n",
      "  \"parts.part_1.coordinate_system.Euler Angles[2]\": 0.0,\n",
      "  \"parts.part_1.coordinate_system.Translation Vector[0]\": 0.0,\n",
      "  \"parts.part_1.coordinate_system.Translation Vector[1]\": 0.0,\n",
      " ...\n",
      "================================================================================\n",
      "‚úÖ Extraction dataset created: 1500 samples\n",
      "\n",
      "üìä Example statistics:\n",
      "   Total parameter count: 2286\n",
      "   Non-zero parameters: 118\n",
      "   Padding ratio: 94.8%\n",
      "\n",
      "   First assistant response preview (first 300 chars):\n",
      "{\n",
      "  \"parts.part_1.coordinate_system.Euler Angles[0]\": 0.0,\n",
      "  \"parts.part_1.coordinate_system.Euler Angles[1]\": 0.0,\n",
      "  \"parts.part_1.coordinate_system.Euler Angles[2]\": 0.0,\n",
      "  \"parts.part_1.coordinate_system.Translation Vector[0]\": 0.0,\n",
      "  \"parts.part_1.coordinate_system.Translation Vector[1]\": 0.0,\n",
      " ...\n"
     ]
    }
   ],
   "source": [
    "# System prompt for parameter prediction task\n",
    "SYSTEM_PROMPT_EXTRACTION = (\n",
    "    \"You are a CAD parameter predictor. Given a natural language instruction, \"\n",
    "    \"predict the appropriate values for CAD parameters. \"\n",
    "    \"Output JSON with parameter paths as keys and predicted values. \"\n",
    "    \"Set parameters to 0 when they are not relevant to the instruction. \"\n",
    "    \"Infer reasonable defaults when specific values are not mentioned.\"\n",
    ")\n",
    "\n",
    "def build_extraction_training_dataset(source_dataset, all_param_paths=None, use_padding=True):\n",
    "    \"\"\"\n",
    "    Create instruction‚Üíparameter pairs for training a parameter prediction model.\n",
    "    \n",
    "    Args:\n",
    "        source_dataset: HuggingFace dataset with 'annotation' and 'json_desc' fields\n",
    "        all_param_paths: Set of all parameter paths for padding (if None, collected automatically)\n",
    "        use_padding: If True, pad missing parameters with 0\n",
    "    \n",
    "    Training format:\n",
    "        - INPUT: annotation (natural language instruction)\n",
    "        - OUTPUT: ONLY extracted parameters as JSON (no name, no annotation)\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    \n",
    "    # If padding requested but no paths provided, collect them first\n",
    "    if use_padding and all_param_paths is None:\n",
    "        print(\"  Collecting all parameter paths for padding...\")\n",
    "        all_param_paths = set()\n",
    "        for example in source_dataset:\n",
    "            json_desc = example.get(\"json_desc\", {})\n",
    "            params = extract_all_parameters_from_sequence(json_desc)\n",
    "            all_param_paths.update(params.keys())\n",
    "        print(f\"  Found {len(all_param_paths)} unique parameter paths\")\n",
    "    \n",
    "    # Build training records\n",
    "    for idx, example in enumerate(source_dataset):\n",
    "        # INPUT: Use annotation as the instruction\n",
    "        instruction = example.get(\"annotation\", \"Describe the CAD model in detail.\")\n",
    "        \n",
    "        # OUTPUT: Extract parameters from json_desc (NOT including name or annotation)\n",
    "        json_desc = example.get(\"json_desc\", {})\n",
    "        parameters = extract_all_parameters_from_sequence(json_desc)\n",
    "        \n",
    "        # Apply padding if requested\n",
    "        if use_padding and all_param_paths:\n",
    "            padded_parameters = OrderedDict()\n",
    "            for path in sorted(all_param_paths):\n",
    "                # Use existing value or pad with 0\n",
    "                padded_parameters[path] = parameters.get(path, 0)\n",
    "            parameters = padded_parameters\n",
    "\n",
    "        # Training record: system prompt + user instruction ‚Üí assistant JSON (parameters ONLY)\n",
    "        records.append(\n",
    "            {\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"system\", \"content\": SYSTEM_PROMPT_EXTRACTION},\n",
    "                    {\"role\": \"user\", \"content\": instruction},\n",
    "                    {\"role\": \"assistant\", \"content\": json.dumps(parameters, indent=2)}\n",
    "                ],\n",
    "                \"metadata\": {\n",
    "                    \"flattened_parameter_count\": len(parameters),\n",
    "                    \"non_zero_parameters\": sum(1 for v in parameters.values() if v != 0),\n",
    "                    \"source_name\": example.get(\"name\", \"unknown\")  # Only for tracking, not in output\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        if (idx + 1) % 300 == 0:\n",
    "            print(f\"  Processed {idx + 1}/{len(source_dataset)} examples...\")\n",
    "\n",
    "    return Dataset.from_list(records)\n",
    "\n",
    "print(\"Building parameter prediction training dataset...\")\n",
    "print(\"üìå Using 'json_desc' field (not 'sequence')\")\n",
    "print(\"üìå OUTPUT: Predicted parameter values as JSON (NO name, NO annotation)\")\n",
    "print(\"üìå STRATEGY: Model learns which parameters are relevant (non-zero) vs irrelevant (0)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Build with padding enabled (pads unused parameters with 0)\n",
    "extraction_dataset = build_extraction_training_dataset(dataset, use_padding=True)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(f\"‚úÖ Extraction dataset created: {len(extraction_dataset)} samples\")\n",
    "print(f\"\\nüìä Example statistics:\")\n",
    "print(f\"   Total parameter count: {extraction_dataset[0]['metadata']['flattened_parameter_count']}\")\n",
    "print(f\"   Non-zero parameters: {extraction_dataset[0]['metadata']['non_zero_parameters']}\")\n",
    "print(f\"   Padding ratio: {100 * (1 - extraction_dataset[0]['metadata']['non_zero_parameters'] / extraction_dataset[0]['metadata']['flattened_parameter_count']):.1f}%\")\n",
    "print(f\"\\n   First assistant response preview (first 300 chars):\")\n",
    "print(extraction_dataset[0]['messages'][2]['content'][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2f519ef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "‚úÖ Extraction dataset created: 1500 samples\n",
      "\n",
      "üìä Example statistics:\n",
      "   Total parameter count: 2286\n",
      "   Non-zero parameters: 25\n",
      "   Padding ratio: 98.9%\n",
      "\n",
      "   First assistant response preview (first 300 chars):\n",
      "{\n",
      "  \"parts.part_1.coordinate_system.Euler Angles[0]\": 0.0,\n",
      "  \"parts.part_1.coordinate_system.Euler Angles[1]\": 0.0,\n",
      "  \"parts.part_1.coordinate_system.Euler Angles[2]\": 0.0,\n",
      "  \"parts.part_1.coordinate_system.Translation Vector[0]\": 0.0,\n",
      "  \"parts.part_1.coordinate_system.Translation Vector[1]\": 0.0,\n",
      " ...\n"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 80)\n",
    "print(f\"‚úÖ Extraction dataset created: {len(extraction_dataset)} samples\")\n",
    "print(f\"\\nüìä Example statistics:\")\n",
    "print(f\"   Total parameter count: {extraction_dataset[4]['metadata']['flattened_parameter_count']}\")\n",
    "print(f\"   Non-zero parameters: {extraction_dataset[4]['metadata']['non_zero_parameters']}\")\n",
    "print(f\"   Padding ratio: {100 * (1 - extraction_dataset[4]['metadata']['non_zero_parameters'] / extraction_dataset[4]['metadata']['flattened_parameter_count']):.1f}%\")\n",
    "print(f\"\\n   First assistant response preview (first 300 chars):\")\n",
    "print(extraction_dataset[4]['messages'][2]['content'][:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b76192",
   "metadata": {},
   "source": [
    "### Analyze Parameter Lengths and Model Capacity\n",
    "\n",
    "Verify that Phi-3-mini-128k can handle the parameter counts and token lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b984f860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PARAMETER LENGTH ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üîç Scanning all 1500 samples...\n",
      "  Processed 300/1500 samples...\n",
      "  Processed 600/1500 samples...\n",
      "  Processed 900/1500 samples...\n",
      "  Processed 1200/1500 samples...\n",
      "  Processed 1500/1500 samples...\n",
      "\n",
      "‚úÖ Analysis complete!\n",
      "\n",
      "üìä DATASET STATISTICS:\n",
      "  Total samples: 1500\n",
      "  Samples with json_desc data: 1500\n",
      "  Unique parameter paths: 2286\n",
      "  Max parameters in single example: 298\n",
      "  Max JSON string length: 17,843 characters\n",
      "  Estimated max tokens (JSON only): ~4,460 tokens\n",
      "  Estimated total tokens per example: ~4,660 tokens\n",
      "\n",
      "üß† MODEL CAPACITY CHECK:\n",
      "  Phi-3-mini-128k context window: 128,000 tokens\n",
      "  Max example size: ~4,660 tokens\n",
      "  Utilization: 3.64%\n",
      "  ‚úÖ Phi-3-mini-128k can EASILY handle this data!\n",
      "\n",
      "üîù TOP 20 MOST COMMON PARAMETERS:\n",
      "   1. parts.part_1.coordinate_system.Euler Angles[0]               | 1500 samples (100.0%)\n",
      "   2. parts.part_1.coordinate_system.Euler Angles[1]               | 1500 samples (100.0%)\n",
      "   3. parts.part_1.coordinate_system.Euler Angles[2]               | 1500 samples (100.0%)\n",
      "   4. parts.part_1.coordinate_system.Translation Vector[0]         | 1500 samples (100.0%)\n",
      "   5. parts.part_1.coordinate_system.Translation Vector[1]         | 1500 samples (100.0%)\n",
      "   6. parts.part_1.coordinate_system.Translation Vector[2]         | 1500 samples (100.0%)\n",
      "   7. parts.part_1.extrusion.extrude_depth_towards_normal          | 1500 samples (100.0%)\n",
      "   8. parts.part_1.extrusion.extrude_depth_opposite_normal         | 1500 samples (100.0%)\n",
      "   9. parts.part_1.extrusion.sketch_scale                          | 1500 samples (100.0%)\n",
      "  10. parts.part_1.extrusion.operation                             | 1500 samples (100.0%)\n",
      "  11. parts.part_1.description.name                                | 1500 samples (100.0%)\n",
      "  12. parts.part_1.description.shape                               | 1500 samples (100.0%)\n",
      "  13. parts.part_1.description.length                              | 1500 samples (100.0%)\n",
      "  14. parts.part_1.description.width                               | 1500 samples (100.0%)\n",
      "  15. parts.part_1.description.height                              | 1500 samples (100.0%)\n",
      "  16. parts.part_1.sketch.face_1.loop_1.line_1.Start Point[0]      | 1062 samples ( 70.8%)\n",
      "  17. parts.part_1.sketch.face_1.loop_1.line_1.Start Point[1]      | 1062 samples ( 70.8%)\n",
      "  18. parts.part_1.sketch.face_1.loop_1.line_1.End Point[0]        | 1062 samples ( 70.8%)\n",
      "  19. parts.part_1.sketch.face_1.loop_1.line_1.End Point[1]        | 1062 samples ( 70.8%)\n",
      "  20. parts.part_1.sketch.face_1.loop_1.line_2.Start Point[0]      | 1053 samples ( 70.2%)\n",
      "\n",
      "================================================================================\n",
      "  Processed 900/1500 samples...\n",
      "  Processed 1200/1500 samples...\n",
      "  Processed 1500/1500 samples...\n",
      "\n",
      "‚úÖ Analysis complete!\n",
      "\n",
      "üìä DATASET STATISTICS:\n",
      "  Total samples: 1500\n",
      "  Samples with json_desc data: 1500\n",
      "  Unique parameter paths: 2286\n",
      "  Max parameters in single example: 298\n",
      "  Max JSON string length: 17,843 characters\n",
      "  Estimated max tokens (JSON only): ~4,460 tokens\n",
      "  Estimated total tokens per example: ~4,660 tokens\n",
      "\n",
      "üß† MODEL CAPACITY CHECK:\n",
      "  Phi-3-mini-128k context window: 128,000 tokens\n",
      "  Max example size: ~4,660 tokens\n",
      "  Utilization: 3.64%\n",
      "  ‚úÖ Phi-3-mini-128k can EASILY handle this data!\n",
      "\n",
      "üîù TOP 20 MOST COMMON PARAMETERS:\n",
      "   1. parts.part_1.coordinate_system.Euler Angles[0]               | 1500 samples (100.0%)\n",
      "   2. parts.part_1.coordinate_system.Euler Angles[1]               | 1500 samples (100.0%)\n",
      "   3. parts.part_1.coordinate_system.Euler Angles[2]               | 1500 samples (100.0%)\n",
      "   4. parts.part_1.coordinate_system.Translation Vector[0]         | 1500 samples (100.0%)\n",
      "   5. parts.part_1.coordinate_system.Translation Vector[1]         | 1500 samples (100.0%)\n",
      "   6. parts.part_1.coordinate_system.Translation Vector[2]         | 1500 samples (100.0%)\n",
      "   7. parts.part_1.extrusion.extrude_depth_towards_normal          | 1500 samples (100.0%)\n",
      "   8. parts.part_1.extrusion.extrude_depth_opposite_normal         | 1500 samples (100.0%)\n",
      "   9. parts.part_1.extrusion.sketch_scale                          | 1500 samples (100.0%)\n",
      "  10. parts.part_1.extrusion.operation                             | 1500 samples (100.0%)\n",
      "  11. parts.part_1.description.name                                | 1500 samples (100.0%)\n",
      "  12. parts.part_1.description.shape                               | 1500 samples (100.0%)\n",
      "  13. parts.part_1.description.length                              | 1500 samples (100.0%)\n",
      "  14. parts.part_1.description.width                               | 1500 samples (100.0%)\n",
      "  15. parts.part_1.description.height                              | 1500 samples (100.0%)\n",
      "  16. parts.part_1.sketch.face_1.loop_1.line_1.Start Point[0]      | 1062 samples ( 70.8%)\n",
      "  17. parts.part_1.sketch.face_1.loop_1.line_1.Start Point[1]      | 1062 samples ( 70.8%)\n",
      "  18. parts.part_1.sketch.face_1.loop_1.line_1.End Point[0]        | 1062 samples ( 70.8%)\n",
      "  19. parts.part_1.sketch.face_1.loop_1.line_1.End Point[1]        | 1062 samples ( 70.8%)\n",
      "  20. parts.part_1.sketch.face_1.loop_1.line_2.Start Point[0]      | 1053 samples ( 70.2%)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Analyze json_desc parameter lengths across the entire dataset\n",
    "print(\"=\" * 80)\n",
    "print(\"PARAMETER LENGTH ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Collect all unique parameter paths and their value types\n",
    "all_parameter_paths = set()\n",
    "parameter_stats = {}\n",
    "max_params_count = 0\n",
    "max_json_length = 0\n",
    "samples_with_data = 0\n",
    "\n",
    "print(\"\\nüîç Scanning all 1500 samples...\")\n",
    "for idx, example in enumerate(dataset):\n",
    "    json_desc = example.get('json_desc', None)\n",
    "    \n",
    "    if json_desc:\n",
    "        samples_with_data += 1\n",
    "        params = extract_all_parameters_from_sequence(json_desc)\n",
    "        \n",
    "        # Track statistics\n",
    "        param_count = len(params)\n",
    "        max_params_count = max(max_params_count, param_count)\n",
    "        \n",
    "        # Track JSON length\n",
    "        json_str = json.dumps(params)\n",
    "        max_json_length = max(max_json_length, len(json_str))\n",
    "        \n",
    "        # Collect all parameter paths\n",
    "        for path, value in params.items():\n",
    "            all_parameter_paths.add(path)\n",
    "            \n",
    "            if path not in parameter_stats:\n",
    "                parameter_stats[path] = {\n",
    "                    'count': 0,\n",
    "                    'types': set(),\n",
    "                    'max_length': 0\n",
    "                }\n",
    "            \n",
    "            parameter_stats[path]['count'] += 1\n",
    "            parameter_stats[path]['types'].add(type(value).__name__)\n",
    "            \n",
    "            # Track max length for string/list values\n",
    "            if isinstance(value, (str, list)):\n",
    "                parameter_stats[path]['max_length'] = max(\n",
    "                    parameter_stats[path]['max_length'], \n",
    "                    len(str(value))\n",
    "                )\n",
    "    \n",
    "    if (idx + 1) % 300 == 0:\n",
    "        print(f\"  Processed {idx + 1}/1500 samples...\")\n",
    "\n",
    "print(f\"\\n‚úÖ Analysis complete!\")\n",
    "print(f\"\\nüìä DATASET STATISTICS:\")\n",
    "print(f\"  Total samples: {len(dataset)}\")\n",
    "print(f\"  Samples with json_desc data: {samples_with_data}\")\n",
    "print(f\"  Unique parameter paths: {len(all_parameter_paths)}\")\n",
    "print(f\"  Max parameters in single example: {max_params_count}\")\n",
    "print(f\"  Max JSON string length: {max_json_length:,} characters\")\n",
    "\n",
    "# Estimate token count (rough: ~4 chars per token)\n",
    "estimated_max_tokens = max_json_length // 4\n",
    "print(f\"  Estimated max tokens (JSON only): ~{estimated_max_tokens:,} tokens\")\n",
    "\n",
    "# Add system prompt + instruction overhead\n",
    "avg_instruction_length = 100  # Average annotation length\n",
    "overhead_tokens = 200  # System prompt + formatting\n",
    "total_estimated_tokens = estimated_max_tokens + overhead_tokens\n",
    "print(f\"  Estimated total tokens per example: ~{total_estimated_tokens:,} tokens\")\n",
    "\n",
    "print(f\"\\nüß† MODEL CAPACITY CHECK:\")\n",
    "phi3_context = 128000\n",
    "print(f\"  Phi-3-mini-128k context window: {phi3_context:,} tokens\")\n",
    "print(f\"  Max example size: ~{total_estimated_tokens:,} tokens\")\n",
    "print(f\"  Utilization: {100 * total_estimated_tokens / phi3_context:.2f}%\")\n",
    "\n",
    "if total_estimated_tokens < phi3_context:\n",
    "    print(f\"  ‚úÖ Phi-3-mini-128k can EASILY handle this data!\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è May need truncation or chunking\")\n",
    "\n",
    "# Show most common parameters\n",
    "print(f\"\\nüîù TOP 20 MOST COMMON PARAMETERS:\")\n",
    "sorted_params = sorted(parameter_stats.items(), key=lambda x: x[1]['count'], reverse=True)\n",
    "for i, (path, stats) in enumerate(sorted_params[:20], 1):\n",
    "    coverage = 100 * stats['count'] / samples_with_data\n",
    "    print(f\"  {i:2d}. {path[:60]:60s} | {stats['count']:4d} samples ({coverage:5.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cd4adb",
   "metadata": {},
   "source": [
    "### Validate Round-Trip Conversion\n",
    "\n",
    "Ensure flatten ‚Üí unflatten preserves the original CAD structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ea7bfb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled 20 CAD examples\n",
      "Max flattened parameter count in sample: 163\n",
      "‚úÖ Flatten ‚Üí unflatten round-trip matches original sequences for sampled examples\n"
     ]
    }
   ],
   "source": [
    "# Quick validation: ensure flattened parameters round-trip to the original CAD sequence\n",
    "def _canonical_json(value):\n",
    "    return json.dumps(value, sort_keys=True, separators=(\",\", \":\"), ensure_ascii=False)\n",
    "\n",
    "sample_size = min(20, len(dataset))\n",
    "indices = list(range(sample_size))\n",
    "round_trip_failures = []\n",
    "max_parameter_count = 0\n",
    "\n",
    "for idx in indices:\n",
    "    example = dataset[idx]\n",
    "    # Use json_desc (the correct field with CAD data)\n",
    "    normalized = _normalize_sequence(example.get(\"json_desc\", {}))\n",
    "    flattened = extract_all_parameters_from_sequence(example.get(\"json_desc\", {}))\n",
    "    reconstructed = unflatten_parameters(flattened)\n",
    "\n",
    "    max_parameter_count = max(max_parameter_count, len(flattened))\n",
    "\n",
    "    if _canonical_json(normalized) != _canonical_json(reconstructed):\n",
    "        round_trip_failures.append(idx)\n",
    "\n",
    "print(f\"Sampled {len(indices)} CAD examples\")\n",
    "print(f\"Max flattened parameter count in sample: {max_parameter_count}\")\n",
    "if round_trip_failures:\n",
    "    print(f\"‚ùå Round-trip mismatch on indices: {round_trip_failures[:5]}\")\n",
    "else:\n",
    "    print(\"‚úÖ Flatten ‚Üí unflatten round-trip matches original sequences for sampled examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f737700f",
   "metadata": {},
   "source": [
    "### Verify Training Data Format\n",
    "\n",
    "Confirm output contains ONLY extracted parameters (no name, no annotation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6af82cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING DATA FORMAT VERIFICATION\n",
      "================================================================================\n",
      "\n",
      "üîç TRAINING CONVERSATION STRUCTURE:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "1Ô∏è‚É£ SYSTEM PROMPT:\n",
      "You are a CAD parameter predictor. Given a natural language instruction, predict the appropriate values for CAD parameters. Output JSON with parameter paths as keys and predicted values. Set parameters to 0 when they are not relevant to the instruction. Infer reasonable defaults when specific values are not mentioned.\n",
      "\n",
      "2Ô∏è‚É£ USER INPUT (annotation from CADmium):\n",
      "Begin by creating a rectangular prism with overall dimensions 0.75 long, 0.375 wide, and 0.46875 high. \n",
      "\n",
      "Next, modify the ends of the prism as follows:\n",
      "\n",
      "At each of the four vertical corners (both at x=0 and x=0.75 along the length), replace the sharp edge with a quarter-circle arc of radius 0.1875, centered horizontally and vertically on the face. The top and bottom vertical edges on the short faces (width sides) are thus rounded, blending tangent to both edge and face.\n",
      "\n",
      "On the top and bottom faces, ensure each end describes a smooth semicircular extension: extrude the width at both ends (x=0 and x=0.75) into a half-cylinder, each with a radius 0.1875 and center at (0.1875, 0.1875) for x=0 and (0.5625, 0.1875) for x=0.75. The total length from tip to tip, including both half-cylindrical ends, is 0.75.\n",
      "\n",
      "Blend all transitions smoothly so the entire object forms a rectangular solid with full width, but with its ends fully rounded in both plan and elevation, producing a pill-shaped bar with rectangular cross-section and semicircular ends on both main faces and edges. All features should be symmetrical about the object‚Äôs longitudinal axis.\n",
      "\n",
      "3Ô∏è‚É£ ASSISTANT OUTPUT (ONLY extracted parameters - NO name, NO annotation):\n",
      "{\n",
      "  \"parts.part_1.coordinate_system.Euler Angles[0]\": 0.0,\n",
      "  \"parts.part_1.coordinate_system.Euler Angles[1]\": 0.0,\n",
      "  \"parts.part_1.coordinate_system.Euler Angles[2]\": 0.0,\n",
      "  \"parts.part_1.coordinate_system.Translation Vector[0]\": 0.0,\n",
      "  \"parts.part_1.coordinate_system.Translation Vector[1]\": 0.0,\n",
      "  \"parts.part_1.coordinate_system.Translation Vector[2]\": 0.0,\n",
      "  \"parts.part_1.description.height\": 0.46874999999999994,\n",
      "  \"parts.part_1.description.length\": 0.75,\n",
      "  \"parts.part_1.description.name\": \"\"\n",
      "...\n",
      "\n",
      "Total length: 141585 characters\n",
      "\n",
      "4Ô∏è‚É£ VERIFICATION:\n",
      "   Output is valid JSON: ‚úÖ\n",
      "   Contains 'name' field: ‚úÖ NO (correct)\n",
      "   Contains 'annotation' field: ‚úÖ NO (correct)\n",
      "   Total parameter keys: 2286\n",
      "   Non-zero parameters: 118\n",
      "\n",
      "   Sample output keys (first 10):\n",
      "     parts.part_1.coordinate_system.Euler Angles[0]: 0.0\n",
      "     parts.part_1.coordinate_system.Euler Angles[1]: 0.0\n",
      "     parts.part_1.coordinate_system.Euler Angles[2]: 0.0\n",
      "     parts.part_1.coordinate_system.Translation Vector[0]: 0.0\n",
      "     parts.part_1.coordinate_system.Translation Vector[1]: 0.0\n",
      "     parts.part_1.coordinate_system.Translation Vector[2]: 0.0\n",
      "     parts.part_1.description.height: 0.46874999999999994\n",
      "     parts.part_1.description.length: 0.75\n",
      "     parts.part_1.description.name: \n",
      "     parts.part_1.description.shape: \n",
      "\n",
      "‚úÖ CORRECT: Output contains ONLY extracted parameters!\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Confirm the training format: output should be ONLY parameters (no name, no annotation)\n",
    "print(\"=\" * 80)\n",
    "print(\"TRAINING DATA FORMAT VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "example = extraction_dataset[0]\n",
    "\n",
    "print(\"\\nüîç TRAINING CONVERSATION STRUCTURE:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ SYSTEM PROMPT:\")\n",
    "print(example['messages'][0]['content'])\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ USER INPUT (annotation from CADmium):\")\n",
    "print(example['messages'][1]['content'])\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ ASSISTANT OUTPUT (ONLY extracted parameters - NO name, NO annotation):\")\n",
    "assistant_output = example['messages'][2]['content']\n",
    "print(assistant_output[:500])\n",
    "print(\"...\")\n",
    "print(f\"\\nTotal length: {len(assistant_output)} characters\")\n",
    "\n",
    "# Parse and verify structure\n",
    "print(\"\\n4Ô∏è‚É£ VERIFICATION:\")\n",
    "try:\n",
    "    parsed = json.loads(assistant_output)\n",
    "    \n",
    "    # Check that output contains ONLY parameters (no 'name' or 'annotation' keys)\n",
    "    has_name = 'name' in parsed\n",
    "    has_annotation = 'annotation' in parsed\n",
    "    \n",
    "    print(f\"   Output is valid JSON: ‚úÖ\")\n",
    "    print(f\"   Contains 'name' field: {'‚ùå FOUND (should not be there!)' if has_name else '‚úÖ NO (correct)'}\")\n",
    "    print(f\"   Contains 'annotation' field: {'‚ùå FOUND (should not be there!)' if has_annotation else '‚úÖ NO (correct)'}\")\n",
    "    print(f\"   Total parameter keys: {len(parsed)}\")\n",
    "    print(f\"   Non-zero parameters: {sum(1 for v in parsed.values() if v != 0)}\")\n",
    "    \n",
    "    # Show sample of what IS in the output\n",
    "    print(f\"\\n   Sample output keys (first 10):\")\n",
    "    for i, key in enumerate(list(parsed.keys())[:10]):\n",
    "        print(f\"     {key}: {parsed[key]}\")\n",
    "    \n",
    "    if not has_name and not has_annotation:\n",
    "        print(f\"\\n‚úÖ CORRECT: Output contains ONLY extracted parameters!\")\n",
    "    else:\n",
    "        print(f\"\\n‚ö†Ô∏è WARNING: Output contains fields that should not be there!\")\n",
    "        \n",
    "except json.JSONDecodeError as e:\n",
    "    print(f\"   ‚ùå Invalid JSON: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8adf45",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚úÖ Section 2 Complete - Ready for Training\n",
    "\n",
    "**Execution Order (CORRECT):**\n",
    "1. ‚úÖ Load CADmium dataset (1,500 samples)\n",
    "2. ‚úÖ Define helper functions (flatten, unflatten, extract)\n",
    "3. ‚úÖ Build extraction dataset with padding\n",
    "4. ‚úÖ Analyze parameter lengths and verify model capacity\n",
    "5. ‚úÖ Validate round-trip conversion (flatten ‚Üí unflatten)\n",
    "6. ‚úÖ Verify training data format\n",
    "\n",
    "**Training Format:**\n",
    "- **Input**: Natural language annotation\n",
    "- **Output**: **Predicted parameter values as JSON** (no name, no annotation)\n",
    "- **Padding Strategy**: Missing parameters filled with 0 to teach relevance\n",
    "- **Model Learning**: Which parameters matter + appropriate values + defaults\n",
    "\n",
    "**Next:** Configure LoRA and start training!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f24e630",
   "metadata": {},
   "source": [
    "## 3. Configure LoRA\n",
    "\n",
    "LoRA allows us to fine-tune large models efficiently by adding trainable low-rank adapters to attention and MLP layers while keeping the base model frozen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d4ee18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LoRA config created\n",
      "   Rank: 16\n",
      "   Alpha: 16\n",
      "   Dropout: 0.05\n",
      "   Target modules: {'up_proj', 'v_proj', 'q_proj', 'k_proj', 'down_proj', 'gate_proj', 'o_proj'}\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank - controls adapter capacity (16-32 recommended)\n",
    "    lora_alpha=16,  # Scaling factor (usually equal to r)\n",
    "    lora_dropout=0.05,  # Dropout for regularization\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # Target all attention and MLP modules for comprehensive adaptation\n",
    "    target_modules=[\n",
    "        \"q_proj\",    # Query projection\n",
    "        \"k_proj\",    # Key projection\n",
    "        \"v_proj\",    # Value projection\n",
    "        \"o_proj\",    # Output projection\n",
    "        \"gate_proj\", # MLP gate\n",
    "        \"up_proj\",   # MLP up projection\n",
    "        \"down_proj\"  # MLP down projection\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LoRA config created\")\n",
    "print(f\"   Rank: {lora_config.r}\")\n",
    "print(f\"   Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"   Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"   Target modules: {lora_config.target_modules}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bc6bf1",
   "metadata": {},
   "source": [
    "## 4. Load Base Model and Tokenizer with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "76c13fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from microsoft/Phi-3-mini-128k-instruct...\n",
      "‚úÖ Tokenizer loaded\n",
      "   Vocab size: 32011\n",
      "   Pad token: <|endoftext|>\n",
      "   EOS token: <|endoftext|>\n",
      "‚úÖ Tokenizer loaded\n",
      "   Vocab size: 32011\n",
      "   Pad token: <|endoftext|>\n",
      "   EOS token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "print(f\"Loading tokenizer from {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\",  # Required for training\n",
    ")\n",
    "\n",
    "# Set padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"‚úÖ Tokenizer loaded\")\n",
    "print(f\"   Vocab size: {len(tokenizer)}\")\n",
    "print(f\"   Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"   EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85f0bd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model microsoft/Phi-3-mini-128k-instruct...\n",
      "‚è≥ This may take a few minutes...\n",
      "‚úÖ Using bfloat16\n",
      "‚úÖ Using bfloat16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "715ec0216c2641459692fd41bb5bc2f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded in torch.bfloat16\n",
      "\n",
      "‚úÖ LoRA adapters added\n",
      "   Trainable params: 8,912,896 (0.23%)\n",
      "   Total params: 3,829,992,448\n",
      "\n",
      "‚úÖ LoRA adapters added\n",
      "   Trainable params: 8,912,896 (0.23%)\n",
      "   Total params: 3,829,992,448\n"
     ]
    }
   ],
   "source": [
    "# Load model in standard precision\n",
    "print(f\"Loading model {model_name}...\")\n",
    "print(\"‚è≥ This may take a few minutes...\")\n",
    "\n",
    "# Determine best dtype\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        _ = torch.zeros(1, dtype=torch.bfloat16, device='cuda')\n",
    "        model_dtype = torch.bfloat16\n",
    "        print(\"‚úÖ Using bfloat16\")\n",
    "    except:\n",
    "        model_dtype = torch.float16\n",
    "        print(\"‚úÖ Using float16\")\n",
    "else:\n",
    "    model_dtype = torch.float32\n",
    "    print(\"‚úÖ Using float32 (CPU mode)\")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=model_dtype,\n",
    "    attn_implementation=\"eager\",\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Model loaded in {model_dtype}\")\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\n‚úÖ LoRA adapters added\")\n",
    "print(f\"   Trainable params: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "print(f\"   Total params: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13941392",
   "metadata": {},
   "source": [
    "## 5. Configure Training with SFT (Supervised Fine-Tuning)\n",
    "\n",
    "Following the recommendations:\n",
    "- Learning rate: 2e-4 with cosine schedule\n",
    "- Warmup: 3%\n",
    "- Sequence length: 2-4k tokens\n",
    "- Effective batch size: 256-512 tokens/step\n",
    "- Training: 2-3 epochs with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "56099795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training configuration created (extraction stage)\n",
      "   Epochs: 2\n",
      "   Batch size: 1\n",
      "   Gradient accumulation: 8\n",
      "   Effective batch size: 8\n",
      "   Learning rate: 0.0002\n",
      "   LR scheduler: SchedulerType.COSINE\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "output_dir = \"./phi3-cad-loraTwoStage-2\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=2,  # 2-3 epochs recommended\n",
    "    per_device_train_batch_size=1,  # Small batch size for memory efficiency\n",
    "    gradient_accumulation_steps=8,  # Effective batch size = 8\n",
    "    learning_rate=2e-4,  # Recommended for LoRA\n",
    "    lr_scheduler_type=\"cosine\",  # Cosine learning rate schedule\n",
    "    warmup_ratio=0.03,  # 3% warmup\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    fp16=False,  # Use bfloat16 instead\n",
    "    bf16=True,  # Better for training stability\n",
    "    gradient_checkpointing=True,  # Save memory\n",
    "    optim=\"adamw_torch\",  # Standard AdamW optimizer\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard for demo\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training configuration created (extraction stage)\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   LR scheduler: {training_args.lr_scheduler_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1a92f107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up extraction training data formatting...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70ec72e2957541c581f08fab57f7d6ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying formatting function to train dataset:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29945c60ca2e48daac30f717fcd9185f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56bcf606c1bf4f61bdccb2dae8ecce05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extraction trainer initialized\n",
      "   Training samples: 1500\n"
     ]
    }
   ],
   "source": [
    "# Format messages to text using chat template\n",
    "# (Extraction dataset already stores JSON strings in assistant messages)\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"\n",
    "    Format examples for SFTTrainer.\n",
    "    Must return a list of strings (one per example).\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for messages in examples[\"messages\"]:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "print(\"Setting up extraction training data formatting...\")\n",
    "\n",
    "# Initialize SFT Trainer for structured-parameter extraction\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=extraction_dataset,\n",
    "    args=training_args,\n",
    "    formatting_func=formatting_prompts_func,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Extraction trainer initialized\")\n",
    "print(f\"   Training samples: {len(extraction_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b58b83d",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n",
    "\n",
    "Start the LoRA fine-tuning process. This will only train the LoRA adapter weights (~0.5-2% of total parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "502432fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='376' max='376' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [376/376 6:55:31, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.008000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.125600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.997300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.095700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.004800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.933600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.956300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.048800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.981200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.007200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.082100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.870500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.811600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.974600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.924100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.726000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.783400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.844400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.819600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.704800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>0.668100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>0.812200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>0.714000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>0.787500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>0.620900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>0.504100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>0.529300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>0.608900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>0.504300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.482500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>0.417400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>0.485700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>0.417100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>0.516400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>0.575600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>0.463200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>0.491900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>0.594400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>0.501000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.519500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>0.521300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>0.427700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>0.405200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>0.512300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>0.592400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>0.617100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>0.412400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>0.399200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>0.389200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.518500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>0.380800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>0.421400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>0.550600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>0.559300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>0.508600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>0.396800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>0.427800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>0.633200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>0.422100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.411800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>0.388300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>0.365400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>0.402300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>0.441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>0.420900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>0.499100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>0.423700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>0.627100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>0.421500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>0.359700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>0.408900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>0.316400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>0.398700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>0.345500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>0.356800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>0.523200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>0.540100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>0.465600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>0.310200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.445000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>0.409900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>0.497000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>0.373300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>0.228600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>0.290500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>0.341300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>0.364700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>0.452200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>0.373500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.330400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>0.357200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>0.492900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>0.424900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>0.389600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>0.388900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>0.633100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>0.339900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>0.354100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>0.441400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.498700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>0.514200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>0.418900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>0.422100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>0.254700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>0.371600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>0.358100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>0.411000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>0.496500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>0.418000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>0.175300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>0.350900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>0.478100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>0.355400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>0.417900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>0.466000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>0.357300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>0.348900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>0.419000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>0.310400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.495700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>0.323600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>0.275600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>0.514400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>0.400300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>0.287300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>0.400200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>0.361700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>0.318000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>0.362000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.327700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>0.622000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>0.353200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>0.491200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>0.574100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>0.315700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>0.365500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>0.318800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>0.354800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>0.563100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.360600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>0.378600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>0.433300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>0.400300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>0.476600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>0.381700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>0.335700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>0.267800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>0.503800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>0.328800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.340600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>151</td>\n",
       "      <td>0.448800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>152</td>\n",
       "      <td>0.321500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>153</td>\n",
       "      <td>0.345900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>154</td>\n",
       "      <td>0.443700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>155</td>\n",
       "      <td>0.511600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>156</td>\n",
       "      <td>0.401300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>157</td>\n",
       "      <td>0.464300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>158</td>\n",
       "      <td>0.431200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>159</td>\n",
       "      <td>0.425000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.501300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>161</td>\n",
       "      <td>0.452500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>162</td>\n",
       "      <td>0.469400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>163</td>\n",
       "      <td>0.413800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>164</td>\n",
       "      <td>0.363100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>165</td>\n",
       "      <td>0.257700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>166</td>\n",
       "      <td>0.362400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>167</td>\n",
       "      <td>0.258200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>168</td>\n",
       "      <td>0.294000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>169</td>\n",
       "      <td>0.451200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.545000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>171</td>\n",
       "      <td>0.467300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>172</td>\n",
       "      <td>0.406300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>173</td>\n",
       "      <td>0.253300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>174</td>\n",
       "      <td>0.264500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>175</td>\n",
       "      <td>0.449100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>176</td>\n",
       "      <td>0.441000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>177</td>\n",
       "      <td>0.445100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>178</td>\n",
       "      <td>0.421300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>179</td>\n",
       "      <td>0.475400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.425100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>181</td>\n",
       "      <td>0.341300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>182</td>\n",
       "      <td>0.411100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>183</td>\n",
       "      <td>0.301800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>184</td>\n",
       "      <td>0.353100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>185</td>\n",
       "      <td>0.316600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>186</td>\n",
       "      <td>0.466900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>187</td>\n",
       "      <td>0.291300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>188</td>\n",
       "      <td>0.581500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>189</td>\n",
       "      <td>0.342600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.538800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>191</td>\n",
       "      <td>0.483300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>192</td>\n",
       "      <td>0.392000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>193</td>\n",
       "      <td>0.356200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>194</td>\n",
       "      <td>0.338600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>195</td>\n",
       "      <td>0.361000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>196</td>\n",
       "      <td>0.553000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>197</td>\n",
       "      <td>0.344100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>198</td>\n",
       "      <td>0.354700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>199</td>\n",
       "      <td>0.431100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.214800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>201</td>\n",
       "      <td>0.391800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>202</td>\n",
       "      <td>0.609300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>203</td>\n",
       "      <td>0.254100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>204</td>\n",
       "      <td>0.340100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>205</td>\n",
       "      <td>0.348500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>206</td>\n",
       "      <td>0.413900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>207</td>\n",
       "      <td>0.240600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>208</td>\n",
       "      <td>0.352000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>209</td>\n",
       "      <td>0.590300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.316900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>211</td>\n",
       "      <td>0.445800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>212</td>\n",
       "      <td>0.397800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>213</td>\n",
       "      <td>0.427400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>214</td>\n",
       "      <td>0.317100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>215</td>\n",
       "      <td>0.381100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>216</td>\n",
       "      <td>0.270500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>217</td>\n",
       "      <td>0.288000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>218</td>\n",
       "      <td>0.338900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>219</td>\n",
       "      <td>0.358000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.455100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>221</td>\n",
       "      <td>0.425500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>222</td>\n",
       "      <td>0.372900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>223</td>\n",
       "      <td>0.253100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>224</td>\n",
       "      <td>0.450500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>225</td>\n",
       "      <td>0.318500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>226</td>\n",
       "      <td>0.236600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>227</td>\n",
       "      <td>0.250900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>228</td>\n",
       "      <td>0.446300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>229</td>\n",
       "      <td>0.329100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.257100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>231</td>\n",
       "      <td>0.318800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>232</td>\n",
       "      <td>0.476300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>233</td>\n",
       "      <td>0.273300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>234</td>\n",
       "      <td>0.308500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>235</td>\n",
       "      <td>0.428100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>236</td>\n",
       "      <td>0.379300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>237</td>\n",
       "      <td>0.373700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>238</td>\n",
       "      <td>0.398500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>239</td>\n",
       "      <td>0.311100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.347600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>241</td>\n",
       "      <td>0.313500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>242</td>\n",
       "      <td>0.288900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>243</td>\n",
       "      <td>0.292000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>244</td>\n",
       "      <td>0.302500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>245</td>\n",
       "      <td>0.359200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>246</td>\n",
       "      <td>0.343900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>247</td>\n",
       "      <td>0.397900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>248</td>\n",
       "      <td>0.407800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>249</td>\n",
       "      <td>0.482100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.497800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>251</td>\n",
       "      <td>0.452700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>252</td>\n",
       "      <td>0.348700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>253</td>\n",
       "      <td>0.357200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>254</td>\n",
       "      <td>0.400000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>255</td>\n",
       "      <td>0.512600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>256</td>\n",
       "      <td>0.454700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>257</td>\n",
       "      <td>0.419800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>258</td>\n",
       "      <td>0.274600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>259</td>\n",
       "      <td>0.303700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.339600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>261</td>\n",
       "      <td>0.354600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>262</td>\n",
       "      <td>0.350700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>263</td>\n",
       "      <td>0.316000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>264</td>\n",
       "      <td>0.486100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>265</td>\n",
       "      <td>0.324500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>266</td>\n",
       "      <td>0.360500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>267</td>\n",
       "      <td>0.459000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>268</td>\n",
       "      <td>0.354000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>269</td>\n",
       "      <td>0.372400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.433800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>271</td>\n",
       "      <td>0.419500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>272</td>\n",
       "      <td>0.316500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>273</td>\n",
       "      <td>0.423500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>274</td>\n",
       "      <td>0.348200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>275</td>\n",
       "      <td>0.520200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>276</td>\n",
       "      <td>0.295900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>277</td>\n",
       "      <td>0.386200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>278</td>\n",
       "      <td>0.449000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>279</td>\n",
       "      <td>0.526900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.453900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>281</td>\n",
       "      <td>0.395600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>282</td>\n",
       "      <td>0.420800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>283</td>\n",
       "      <td>0.372900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>284</td>\n",
       "      <td>0.319100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>285</td>\n",
       "      <td>0.407900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>286</td>\n",
       "      <td>0.391500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>287</td>\n",
       "      <td>0.367600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>288</td>\n",
       "      <td>0.352000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>289</td>\n",
       "      <td>0.371000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.563300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>291</td>\n",
       "      <td>0.207100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>292</td>\n",
       "      <td>0.433500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>293</td>\n",
       "      <td>0.295500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>294</td>\n",
       "      <td>0.287100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>295</td>\n",
       "      <td>0.466200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>296</td>\n",
       "      <td>0.281000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>297</td>\n",
       "      <td>0.297700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>298</td>\n",
       "      <td>0.322300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>299</td>\n",
       "      <td>0.467800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.346700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>301</td>\n",
       "      <td>0.431000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>302</td>\n",
       "      <td>0.441300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>303</td>\n",
       "      <td>0.460500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>304</td>\n",
       "      <td>0.468000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>305</td>\n",
       "      <td>0.349500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>306</td>\n",
       "      <td>0.363200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>307</td>\n",
       "      <td>0.288800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>308</td>\n",
       "      <td>0.319900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>309</td>\n",
       "      <td>0.504700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.460400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>311</td>\n",
       "      <td>0.343200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>312</td>\n",
       "      <td>0.272700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>313</td>\n",
       "      <td>0.401100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>314</td>\n",
       "      <td>0.315200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>315</td>\n",
       "      <td>0.325700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>316</td>\n",
       "      <td>0.654300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>317</td>\n",
       "      <td>0.379900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>318</td>\n",
       "      <td>0.449800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>319</td>\n",
       "      <td>0.322300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.390100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>321</td>\n",
       "      <td>0.347300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>322</td>\n",
       "      <td>0.382500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>323</td>\n",
       "      <td>0.274800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>324</td>\n",
       "      <td>0.306600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>325</td>\n",
       "      <td>0.582700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>326</td>\n",
       "      <td>0.395000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>327</td>\n",
       "      <td>0.276400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>328</td>\n",
       "      <td>0.409400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>329</td>\n",
       "      <td>0.369900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.390800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>331</td>\n",
       "      <td>0.458200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>332</td>\n",
       "      <td>0.574500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>333</td>\n",
       "      <td>0.425900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>334</td>\n",
       "      <td>0.283200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>335</td>\n",
       "      <td>0.693300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>336</td>\n",
       "      <td>0.273000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>337</td>\n",
       "      <td>0.313000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>338</td>\n",
       "      <td>0.327300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>339</td>\n",
       "      <td>0.270700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.296700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>341</td>\n",
       "      <td>0.361000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>342</td>\n",
       "      <td>0.477200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>343</td>\n",
       "      <td>0.564400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>344</td>\n",
       "      <td>0.505900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>345</td>\n",
       "      <td>0.270200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>346</td>\n",
       "      <td>0.241100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>347</td>\n",
       "      <td>0.396000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>348</td>\n",
       "      <td>0.359300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>349</td>\n",
       "      <td>0.366300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.332000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>351</td>\n",
       "      <td>0.452400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>352</td>\n",
       "      <td>0.376000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>353</td>\n",
       "      <td>0.272500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>354</td>\n",
       "      <td>0.396700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>355</td>\n",
       "      <td>0.333200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>356</td>\n",
       "      <td>0.228800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>357</td>\n",
       "      <td>0.241500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>358</td>\n",
       "      <td>0.379200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>359</td>\n",
       "      <td>0.373200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.521100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>361</td>\n",
       "      <td>0.351900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>362</td>\n",
       "      <td>0.337100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>363</td>\n",
       "      <td>0.202900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>364</td>\n",
       "      <td>0.380900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>365</td>\n",
       "      <td>0.383600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>366</td>\n",
       "      <td>0.565600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>367</td>\n",
       "      <td>0.282200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>368</td>\n",
       "      <td>0.438500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>369</td>\n",
       "      <td>0.352300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.467700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>371</td>\n",
       "      <td>0.343300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>372</td>\n",
       "      <td>0.417500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>373</td>\n",
       "      <td>0.446000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>374</td>\n",
       "      <td>0.371200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>375</td>\n",
       "      <td>0.338100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>376</td>\n",
       "      <td>0.319100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "‚úÖ Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f01414",
   "metadata": {},
   "source": [
    "## 7. Save the LoRA Adapters\n",
    "\n",
    "Save only the trained LoRA adapters (much smaller than the full model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "39defbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Parameter-extraction adapters saved to: ./phi3-cad-TwoStages-Radapters-2\n",
      "\n",
      "You can load these adapters later with:\n",
      "  from peft import PeftModel\n",
      "  base_model = AutoModelForCausalLM.from_pretrained('microsoft/Phi-3-mini-128k-instruct')\n",
      "  model = PeftModel.from_pretrained(base_model, './phi3-cad-TwoStages-Radapters-2')\n"
     ]
    }
   ],
   "source": [
    "# Save LoRA adapters (extraction model)\n",
    "lora_output_dir = \"./phi3-cad-TwoStages-Radapters-2\"\n",
    "\n",
    "model.save_pretrained(lora_output_dir)\n",
    "tokenizer.save_pretrained(lora_output_dir)\n",
    "\n",
    "print(f\"‚úÖ Parameter-extraction adapters saved to: {lora_output_dir}\")\n",
    "print(\"\\nYou can load these adapters later with:\")\n",
    "print(f\"  from peft import PeftModel\")\n",
    "print(f\"  base_model = AutoModelForCausalLM.from_pretrained('{model_name}')\")\n",
    "print(f\"  model = PeftModel.from_pretrained(base_model, '{lora_output_dir}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4abc32",
   "metadata": {},
   "source": [
    "## 8. Test the Parameter Extraction Model\n",
    "\n",
    "Generate structured parameter maps from natural language instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4b78b0",
   "metadata": {},
   "source": [
    "**Important Note on Inference Speed:**\n",
    "\n",
    "The model was trained with **full padding** (all 2,286 parameters in every example), but for **inference**, you don't need to generate all padded parameters!\n",
    "\n",
    "**Why this works:**\n",
    "- ‚úÖ Training with padding taught the model which parameters are relevant\n",
    "- ‚úÖ At inference, the model naturally generates only relevant parameters\n",
    "- ‚úÖ Model will emit EOS token when done (stops early)\n",
    "- ‚úÖ Much faster: ~1-2 minutes instead of 47 minutes!\n",
    "\n",
    "**Token limits:**\n",
    "- Training data: ~35,000 tokens (full padding)\n",
    "- Practical inference: 4,000-8,000 tokens (relevant params only)\n",
    "- Default: 8,192 tokens (generous limit for most CAD instructions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56353281",
   "metadata": {},
   "source": [
    "## üîÑ CRITICAL: Load Trained Adapters for Testing\n",
    "\n",
    "**IMPORTANT**: After training, we need to reload the model with the trained adapters!\n",
    "\n",
    "The model in memory has the LoRA structure, but we need to load the weights that were saved during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb85507f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üîÑ RELOADING MODEL WITH TRAINED ADAPTERS\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ Loading base model: microsoft/Phi-3-mini-128k-instruct\n",
      "\n",
      "1Ô∏è‚É£ Loading base model: microsoft/Phi-3-mini-128k-instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85b8e562ef4848b88b99dd759b7d8e1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Base model loaded\n",
      "\n",
      "2Ô∏è‚É£ Loading trained adapters from: ./phi3-cad-TwoStages-Radapters-2\n",
      "   ‚úÖ Trained adapters loaded!\n",
      "\n",
      "üìä Model Statistics:\n",
      "   Total params: 3,829,992,448\n",
      "   Trainable params: 0 (0.00%)\n",
      "   Device: cuda:0\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Ready for inference with trained adapters!\n",
      "================================================================================\n",
      "   ‚úÖ Trained adapters loaded!\n",
      "\n",
      "üìä Model Statistics:\n",
      "   Total params: 3,829,992,448\n",
      "   Trainable params: 0 (0.00%)\n",
      "   Device: cuda:0\n",
      "\n",
      "================================================================================\n",
      "‚úÖ Ready for inference with trained adapters!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# RELOAD MODEL WITH TRAINED ADAPTERS\n",
    "print(\"=\" * 80)\n",
    "print(\"üîÑ RELOADING MODEL WITH TRAINED ADAPTERS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Clear the current model from memory\n",
    "del model\n",
    "import gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Load fresh base model\n",
    "print(f\"\\n1Ô∏è‚É£ Loading base model: {model_name}\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\",\n",
    ")\n",
    "print(\"   ‚úÖ Base model loaded\")\n",
    "\n",
    "# Load trained LoRA adapters\n",
    "from peft import PeftModel\n",
    "\n",
    "adapter_path = \"./phi3-cad-TwoStages-Radapters-2\"\n",
    "print(f\"\\n2Ô∏è‚É£ Loading trained adapters from: {adapter_path}\")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    adapter_path,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "print(\"   ‚úÖ Trained adapters loaded!\")\n",
    "\n",
    "# Verify\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"   Total params: {total_params:,}\")\n",
    "print(f\"   Trainable params: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "print(f\"   Device: {model.device}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ Ready for inference with trained adapters!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270fd473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extraction inference helper ready\n"
     ]
    }
   ],
   "source": [
    "# Inference helper for parameter extraction\n",
    "def extract_parameters(instruction, max_new_tokens=8192, temperature=0.0, top_p=0.95):\n",
    "    \"\"\"\n",
    "    Run the fine-tuned extractor and return parsed parameter dict.\n",
    "    \n",
    "    Note: max_new_tokens=8192 is a practical limit. Model was trained with padding\n",
    "    (all 2286 params), but at inference we let it generate only relevant parameters.\n",
    "    The model learned which params are relevant, so it will naturally stop early.\n",
    "    \"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT_EXTRACTION},\n",
    "        {\"role\": \"user\", \"content\": instruction},\n",
    "    ]\n",
    "\n",
    "    # Format using chat template\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=(temperature > 0.0),\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=False,\n",
    "        )\n",
    "\n",
    "    # Decode only the newly generated tokens (not the input prompt)\n",
    "    generated_ids = outputs[0][inputs.input_ids.shape[1]:]\n",
    "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    # Try multiple parsing strategies\n",
    "    parsed = None\n",
    "    \n",
    "    # Strategy 1: Direct JSON parse\n",
    "    try:\n",
    "        parsed = json.loads(generated_text)\n",
    "        return parsed\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    # Strategy 2: Find JSON block in output\n",
    "    try:\n",
    "        start = generated_text.find('{')\n",
    "        end = generated_text.rfind('}') + 1\n",
    "        if start != -1 and end > start:\n",
    "            json_text = generated_text[start:end]\n",
    "            parsed = json.loads(json_text)\n",
    "            return parsed\n",
    "    except (json.JSONDecodeError, ValueError):\n",
    "        pass\n",
    "    \n",
    "    # Strategy 3: Split by assistant marker (fallback)\n",
    "    for marker in [\"<|assistant|>\", \"assistant:\", \"Assistant:\"]:\n",
    "        if marker in generated_text:\n",
    "            text = generated_text.split(marker)[-1].strip()\n",
    "            try:\n",
    "                parsed = json.loads(text)\n",
    "                return parsed\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    # If all parsing fails, return debugging info\n",
    "    return {\n",
    "        \"error\": \"parsing_failed\",\n",
    "        \"raw_output\": generated_text[:2000],\n",
    "        \"output_length\": len(generated_text)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Extraction inference helper ready\")#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "28ab792b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing parameter extraction model\n",
      "\n",
      "================================================================================\n",
      "‚è±Ô∏è Note: Using max_new_tokens=8192 for practical inference speed\n",
      "   Model will generate only relevant parameters and stop early with EOS\n",
      "================================================================================\n",
      "\n",
      "üìù Test 1: Create a cube 10mm by 20mm by 30mm centered at the origin\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müìù Test \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m result = \u001b[43mextract_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8192\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33merror\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m result:\n\u001b[32m     20\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ùå Parsing error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult.get(\u001b[33m'\u001b[39m\u001b[33merror\u001b[39m\u001b[33m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 26\u001b[39m, in \u001b[36mextract_parameters\u001b[39m\u001b[34m(instruction, max_new_tokens, temperature, top_p)\u001b[39m\n\u001b[32m     23\u001b[39m inputs = tokenizer(formatted_prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(model.device)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     27\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     28\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     30\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m \u001b[49m\u001b[43m>\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     37\u001b[39m \u001b[38;5;66;03m# Decode only the newly generated tokens (not the input prompt)\u001b[39;00m\n\u001b[32m     38\u001b[39m generated_ids = outputs[\u001b[32m0\u001b[39m][inputs.input_ids.shape[\u001b[32m1\u001b[39m]:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/peft/peft_model.py:1973\u001b[39m, in \u001b[36mPeftModelForCausalLM.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1971\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m   1972\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1973\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1974\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1975\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.base_model.generate(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/transformers/generation/utils.py:2787\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m     outputs = \u001b[43mmodel_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   2790\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   2791\u001b[39m     outputs,\n\u001b[32m   2792\u001b[39m     model_kwargs,\n\u001b[32m   2793\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2794\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi_hyphen_3_hyphen_mini_hyphen_128k_hyphen_instruct/072cb7562cb8c4adf682a8e186aaafa49469eb5d/modeling_phi3.py:1243\u001b[39m, in \u001b[36mPhi3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1240\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m   1242\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1243\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1244\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1255\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1256\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.lm_head(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi_hyphen_3_hyphen_mini_hyphen_128k_hyphen_instruct/072cb7562cb8c4adf682a8e186aaafa49469eb5d/modeling_phi3.py:1121\u001b[39m, in \u001b[36mPhi3Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1111\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m   1112\u001b[39m         decoder_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m   1113\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m         use_cache,\n\u001b[32m   1119\u001b[39m     )\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1124\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1130\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi_hyphen_3_hyphen_mini_hyphen_128k_hyphen_instruct/072cb7562cb8c4adf682a8e186aaafa49469eb5d/modeling_phi3.py:842\u001b[39m, in \u001b[36mPhi3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    841\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m842\u001b[39m attn_outputs, self_attn_weights, present_key_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    851\u001b[39m hidden_states = residual + \u001b[38;5;28mself\u001b[39m.resid_attn_dropout(attn_outputs)\n\u001b[32m    853\u001b[39m residual = hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi_hyphen_3_hyphen_mini_hyphen_128k_hyphen_instruct/072cb7562cb8c4adf682a8e186aaafa49469eb5d/modeling_phi3.py:334\u001b[39m, in \u001b[36mPhi3Attention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[39m\n\u001b[32m    328\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    329\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mThe cache structure has changed since version v4.36. If you are using \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    330\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfor auto-regressive decoding with k/v caching, please make sure to initialize the attention class \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    331\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mwith a layer index.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    332\u001b[39m         )\n\u001b[32m    333\u001b[39m     kv_seq_len += past_key_value.get_usable_length(kv_seq_len, \u001b[38;5;28mself\u001b[39m.layer_idx)\n\u001b[32m--> \u001b[39m\u001b[32m334\u001b[39m cos, sin = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrotary_emb\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkv_seq_len\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    336\u001b[39m query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n\u001b[32m    338\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi_hyphen_3_hyphen_mini_hyphen_128k_hyphen_instruct/072cb7562cb8c4adf682a8e186aaafa49469eb5d/modeling_phi3.py:159\u001b[39m, in \u001b[36mPhi3LongRoPEScaledRotaryEmbedding.forward\u001b[39m\u001b[34m(self, x, position_ids, seq_len)\u001b[39m\n\u001b[32m    156\u001b[39m     ext_factors = torch.tensor(\u001b[38;5;28mself\u001b[39m.short_factor, dtype=torch.float32, device=x.device)\n\u001b[32m    158\u001b[39m inv_freq_shape = torch.arange(\u001b[32m0\u001b[39m, \u001b[38;5;28mself\u001b[39m.dim, \u001b[32m2\u001b[39m, dtype=torch.int64, device=x.device).float() / \u001b[38;5;28mself\u001b[39m.dim\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m \u001b[38;5;28mself\u001b[39m.inv_freq = \u001b[32m1.0\u001b[39m / (ext_factors * \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minv_freq_shape\u001b[49m)\n\u001b[32m    161\u001b[39m inv_freq_expanded = \u001b[38;5;28mself\u001b[39m.inv_freq[\u001b[38;5;28;01mNone\u001b[39;00m, :, \u001b[38;5;28;01mNone\u001b[39;00m].float().expand(position_ids.shape[\u001b[32m0\u001b[39m], -\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m)\n\u001b[32m    162\u001b[39m position_ids_expanded = position_ids[:, \u001b[38;5;28;01mNone\u001b[39;00m, :].float()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/torch/_tensor.py:35\u001b[39m, in \u001b[36m_handle_torch_function_and_wrap_type_error_to_not_implemented.<locals>.wrapped\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_handle_torch_function_and_wrap_type_error_to_not_implemented\u001b[39m(f):\n\u001b[32m     33\u001b[39m     assigned = functools.WRAPPER_ASSIGNMENTS\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[38;5;129m@functools\u001b[39m.wraps(f, assigned=assigned)\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(*args, **kwargs):\n\u001b[32m     37\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     38\u001b[39m             \u001b[38;5;66;03m# See https://github.com/pytorch/pytorch/issues/75462\u001b[39;00m\n\u001b[32m     39\u001b[39m             \u001b[38;5;28;01mif\u001b[39;00m has_torch_function(args):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Test with sample prompts\n",
    "test_prompts = [\n",
    "    \"Create a cube 10mm by 20mm by 30mm centered at the origin\",\n",
    "    \"Design a cylinder with radius 3mm and height 15mm\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing parameter extraction model\\n\")\n",
    "print(\"=\" * 80)\n",
    "print(\"‚è±Ô∏è Note: Using max_new_tokens=8192 for practical inference speed\")\n",
    "print(\"   Model will generate only relevant parameters and stop early with EOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nüìù Test {i}: {prompt}\")\n",
    "    print(\"-\" * 80)\n",
    "\n",
    "    result = extract_parameters(prompt, max_new_tokens=8192)\n",
    "    \n",
    "    if \"error\" in result:\n",
    "        print(f\"‚ùå Parsing error: {result.get('error')}\")\n",
    "        print(f\"Output length: {result.get('output_length', 0)} characters\")\n",
    "        print(f\"Raw output preview:\\n{result.get('raw_output', '')[:500]}\")\n",
    "    else:\n",
    "        print(f\"‚úÖ Successfully extracted {len(result)} parameters\")\n",
    "        print(f\"Non-zero parameters: {sum(1 for v in result.values() if v != 0)}\")\n",
    "        print(\"\\nExtracted parameters (first 20):\")\n",
    "        for j, (k, v) in enumerate(list(result.items())[:20]):\n",
    "            print(f\"  {k}: {v}\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d37504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "PADDED OUTPUT SIZE ANALYSIS\n",
      "================================================================================\n",
      "\n",
      "üìä TRAINING DATA OUTPUT FORMAT:\n",
      "  Total parameters: 2286\n",
      "  Non-zero parameters: 118\n",
      "  Zero-padded parameters: 2168\n",
      "  JSON string length: 141,585 characters\n",
      "  Estimated tokens: ~35,396 tokens\n",
      "\n",
      "üìè REQUIRED max_new_tokens:\n",
      "  Minimum: 35,396 tokens\n",
      "  Recommended (with 20% buffer): 42,475 tokens\n",
      "  Current setting: 512 tokens ‚ùå\n",
      "\n",
      "üîç Sample parameters (first 15):\n",
      "  parts.part_1.coordinate_system.Euler Angles[0]: 0.0\n",
      "  parts.part_1.coordinate_system.Euler Angles[1]: 0.0\n",
      "  parts.part_1.coordinate_system.Euler Angles[2]: 0.0\n",
      "  parts.part_1.coordinate_system.Translation Vector[0]: 0.0\n",
      "  parts.part_1.coordinate_system.Translation Vector[1]: 0.0\n",
      "  parts.part_1.coordinate_system.Translation Vector[2]: 0.0\n",
      "  parts.part_1.description.height: 0.46874999999999994\n",
      "  parts.part_1.description.length: 0.75\n",
      "  parts.part_1.description.name: \n",
      "  parts.part_1.description.shape: \n",
      "  parts.part_1.description.width: 0.37499999999999994\n",
      "  parts.part_1.extrusion.extrude_depth_opposite_normal: 0.0\n",
      "  parts.part_1.extrusion.extrude_depth_towards_normal: 0.4687\n",
      "  parts.part_1.extrusion.operation: NewBodyFeatureOperation\n",
      "  parts.part_1.extrusion.sketch_scale: 0.75\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Check actual padded output size in training data\n",
    "import json\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"PADDED OUTPUT SIZE ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "first_example = extraction_dataset[0]\n",
    "assistant_output = first_example['messages'][2]['content']\n",
    "parsed_params = json.loads(assistant_output)\n",
    "\n",
    "print(f\"\\nüìä TRAINING DATA OUTPUT FORMAT:\")\n",
    "print(f\"  Total parameters: {len(parsed_params)}\")\n",
    "print(f\"  Non-zero parameters: {sum(1 for v in parsed_params.values() if v != 0)}\")\n",
    "print(f\"  Zero-padded parameters: {sum(1 for v in parsed_params.values() if v == 0)}\")\n",
    "print(f\"  JSON string length: {len(assistant_output):,} characters\")\n",
    "print(f\"  Estimated tokens: ~{len(assistant_output) // 4:,} tokens\")\n",
    "\n",
    "print(f\"\\nüìè REQUIRED max_new_tokens:\")\n",
    "# Add 20% buffer for safety\n",
    "required_tokens = int((len(assistant_output) // 4) * 1.2)\n",
    "print(f\"  Minimum: {len(assistant_output) // 4:,} tokens\")\n",
    "print(f\"  Recommended (with 20% buffer): {required_tokens:,} tokens\")\n",
    "print(f\"  Current setting: 512 tokens ‚ùå\")\n",
    "\n",
    "print(f\"\\nüîç Sample parameters (first 15):\")\n",
    "for i, (key, value) in enumerate(list(parsed_params.items())[:15]):\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb839b0",
   "metadata": {},
   "source": [
    "## 9. Next Step: Deterministic Mapping (Stage 2)\n",
    "\n",
    "The second stage will map the extracted parameter dictionary into a schema-compliant CAD JSON using `cad_model_schema.json`.\n",
    "A helper will:\n",
    "- Unflatten the parameter keys back into nested structures\n",
    "- Apply unit/frame defaults and normalization\n",
    "- Validate against the schema (via `jsonschema`)\n",
    "- Emit the final CAD JSON payload\n",
    "\n",
    "Implementation TBD (will be added after verifying Stage 1 quality)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871582d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ See code comments for loading the extraction adapters\n"
     ]
    }
   ],
   "source": [
    "# Example: How to load the extraction adapters later\n",
    "\"\"\"\n",
    "from peft import PeftModel\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-128k-instruct\",\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "extractor_model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"./phi3-cad-parameter-adapters\"\n",
    ")\n",
    "\n",
    "# To run inference:\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\", trust_remote_code=True)\n",
    "# messages = [\n",
    "#     {\"role\": \"system\", \"content\": SYSTEM_PROMPT_EXTRACTION},\n",
    "#     {\"role\": \"user\", \"content\": instruction},\n",
    "# ]\n",
    "# prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(extractor_model.device)\n",
    "# output = extractor_model.generate(**inputs, max_new_tokens=256)\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚úÖ See code comments for loading the extraction adapters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26ced0a",
   "metadata": {},
   "source": [
    "## Summary & Next Steps\n",
    "\n",
    "### What We've Built\n",
    "‚úÖ **Structured Parameter Extraction LoRA**: Natural language ‚Üí flattened CAD parameter map  \n",
    "‚úÖ **Dataset Builder**: 1.5k-sample extraction dataset covering every CADmium parameter  \n",
    "‚úÖ **Inference Helper**: Quick function to inspect extracted parameter dictionaries  \n",
    "\n",
    "### Key Configurations Used\n",
    "- **Base Model**: microsoft/Phi-3-mini-128k-instruct (long context)\n",
    "- **LoRA Config**: Rank 16, Alpha 16, Dropout 0.05\n",
    "- **Training**: 2 epochs, LR 2e-4, cosine schedule, 3% warmup\n",
    "- **Dataset Size**: 1,500 CADmium samples (shuffled subset)\n",
    "\n",
    "### Recommended Next Steps\n",
    "1. **Stage 2 Mapping**  \n",
    "   - Implement deterministic mapper to convert extracted params ‚Üí CAD JSON  \n",
    "   - Validate against `cad_model_schema.json`\n",
    "2. **Quality Evaluation**  \n",
    "   - Measure extraction accuracy vs. ground-truth parameters  \n",
    "   - Identify frequent gaps (missing fields, unit conversions)\n",
    "3. **Dataset Expansion**  \n",
    "   - Increase sample count to 3-5k if GPU memory allows  \n",
    "   - Augment with synthetic instructions covering edge cases\n",
    "4. **Integration**  \n",
    "   - Wrap both stages in a single inference function  \n",
    "   - Add schema validation + error reporting\n",
    "\n",
    "### Resources\n",
    "- [PEFT Documentation](https://huggingface.co/docs/peft)\n",
    "- [TRL SFTTrainer](https://huggingface.co/docs/trl)\n",
    "- [CADmium Dataset](https://huggingface.co/datasets/chandar-lab/CADmium)\n",
    "- [JSON Schema Validation](https://python-jsonschema.readthedocs.io/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (envfin-416Final)",
   "language": "python",
   "name": "envfin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
