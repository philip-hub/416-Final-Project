{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d327dcfe",
   "metadata": {},
   "source": [
    "## ‚úÖ ALL DEPENDENCY ISSUES FIXED!\n",
    "\n",
    "**Final working versions:**\n",
    "- `transformers`: 4.57.1 ‚Üí **4.44.2** ‚úÖ (has proper DynamicCache support)\n",
    "- `peft`: 0.17.1 ‚Üí **0.13.2** ‚úÖ (supports use_dora, use_rslora, all config params)\n",
    "- **These versions are fully compatible!**\n",
    "\n",
    "**Next steps:**\n",
    "1. **Restart the kernel** (Kernel ‚Üí Restart Kernel) \n",
    "2. Run all cells from the top to reload with correct library versions\n",
    "3. The diagnostic test should complete in ~30-60 seconds with cache enabled\n",
    "4. Full inference (4096 tokens) should take 3-7 minutes\n",
    "\n",
    "**What was the problem?**\n",
    "- ‚ùå transformers 4.57.1 was too new (DynamicCache had breaking changes)\n",
    "- ‚ùå transformers 4.36.0 was too old (missing cache classes peft expected)\n",
    "- ‚ùå peft 0.7.0/0.8.2/0.11.0 were too old (missing use_dora, use_rslora support)\n",
    "- ‚úÖ **Solution**: transformers 4.44.2 + peft 0.13.2 = perfect compatibility!\n",
    "- ‚úÖ Your training and adapters are perfect - this was 100% library version mismatch!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e850a24a",
   "metadata": {},
   "source": [
    "# Test Trained CAD Parameter Extraction Model\n",
    "\n",
    "This notebook loads the trained LoRA adapters from `./phi3-cad-TwoStages-Radapters-2` and tests the model's ability to extract CAD parameters from natural language instructions.\n",
    "\n",
    "**Prerequisites:**\n",
    "- Trained adapters saved in `./phi3-cad-TwoStages-Radapters-2`\n",
    "- GPU with CUDA support (recommended)\n",
    "- Python environment with: `transformers`, `peft`, `torch`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c579b4",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ff70d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Imports successful\n",
      "PyTorch version: 2.4.0+cu121\n",
      "CUDA available: True\n",
      "GPU: Quadro RTX 6000\n",
      "GPU Memory: 25.19 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"‚úÖ Imports successful\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c80178",
   "metadata": {},
   "source": [
    "## 2. Load Base Model and Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d063c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "LOADING MODEL\n",
      "================================================================================\n",
      "\n",
      "1Ô∏è‚É£ Loading tokenizer from microsoft/Phi-3-mini-128k-instruct...\n",
      "   ‚úÖ Tokenizer loaded\n",
      "   Vocab size: 32011\n",
      "   EOS token: <|endoftext|>\n",
      "   ‚úÖ Tokenizer loaded\n",
      "   Vocab size: 32011\n",
      "   EOS token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Model configuration\n",
    "base_model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "adapter_path = \"./phi3-cad-TwoStages-Radapters-2\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"LOADING MODEL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Load tokenizer\n",
    "print(f\"\\n1Ô∏è‚É£ Loading tokenizer from {base_model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    base_model_name,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\"\n",
    ")\n",
    "\n",
    "# Set padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"   ‚úÖ Tokenizer loaded\")\n",
    "print(f\"   Vocab size: {len(tokenizer)}\")\n",
    "print(f\"   EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2740f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2Ô∏è‚É£ Loading base model: microsoft/Phi-3-mini-128k-instruct\n",
      "   ‚è≥ This may take 1-2 minutes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n",
      "Current `flash-attention` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14613f97b8e04c59a4eab6f2efe7b3bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Base model loaded\n",
      "   Device: cuda:0\n",
      "   Dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "# Load base model\n",
    "print(f\"\\n2Ô∏è‚É£ Loading base model: {base_model_name}\")\n",
    "print(\"   ‚è≥ This may take 1-2 minutes...\")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    attn_implementation=\"eager\",\n",
    "    \n",
    ")\n",
    "\n",
    "print(f\"   ‚úÖ Base model loaded\")\n",
    "print(f\"   Device: {base_model.device}\")\n",
    "print(f\"   Dtype: {base_model.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "946b791c",
   "metadata": {},
   "source": [
    "## 3. Load Trained LoRA Adapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "39bf3c13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "3Ô∏è‚É£ Loading trained LoRA adapters from: ./phi3-cad-TwoStages-Radapters-2\n",
      "   ‚è≥ Loading adapters...\n",
      "   ‚úÖ Trained adapters loaded!\n",
      "\n",
      "üìä Model Statistics:\n",
      "   Total parameters: 3,829,992,448\n",
      "   Trainable parameters: 0\n",
      "   Trainable %: 0.00%\n",
      "\n",
      "================================================================================\n",
      "‚úÖ MODEL READY FOR INFERENCE\n",
      "================================================================================\n",
      "   ‚úÖ Trained adapters loaded!\n",
      "\n",
      "üìä Model Statistics:\n",
      "   Total parameters: 3,829,992,448\n",
      "   Trainable parameters: 0\n",
      "   Trainable %: 0.00%\n",
      "\n",
      "================================================================================\n",
      "‚úÖ MODEL READY FOR INFERENCE\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Load trained adapters\n",
    "print(f\"\\n3Ô∏è‚É£ Loading trained LoRA adapters from: {adapter_path}\")\n",
    "print(\"   ‚è≥ Loading adapters...\")\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    adapter_path,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "print(f\"   ‚úÖ Trained adapters loaded!\")\n",
    "\n",
    "# Model statistics\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "print(f\"\\nüìä Model Statistics:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ MODEL READY FOR INFERENCE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47cb4bd",
   "metadata": {},
   "source": [
    "## 4. Define Inference Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89725899",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Inference function defined\n"
     ]
    }
   ],
   "source": [
    "# System prompt used during training\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a CAD parameter predictor. Given a natural language instruction, \"\n",
    "    \"predict the appropriate values for CAD parameters. \"\n",
    "    \"Output JSON with parameter paths as keys and predicted values. \"\n",
    "    \"Set parameters to 0 when they are not relevant to the instruction. \"\n",
    "    \"Infer reasonable defaults when specific values are not mentioned.\"\n",
    ")\n",
    "\n",
    "def extract_parameters(instruction, max_new_tokens=4096, temperature=0.0, top_p=0.95, verbose=True):\n",
    "    \"\"\"\n",
    "    Extract CAD parameters from natural language instruction.\n",
    "    \n",
    "    Args:\n",
    "        instruction: Natural language CAD instruction\n",
    "        max_new_tokens: Maximum tokens to generate (default: 4096)\n",
    "        temperature: Sampling temperature (0.0 = greedy)\n",
    "        top_p: Nucleus sampling parameter\n",
    "        verbose: Print generation progress\n",
    "    \n",
    "    Returns:\n",
    "        dict: Extracted parameters or error info\n",
    "    \"\"\"\n",
    "    if verbose:\n",
    "        print(f\"‚è±Ô∏è  Generating with max_new_tokens={max_new_tokens}...\")\n",
    "    \n",
    "    # Format prompt using chat template\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": instruction},\n",
    "    ]\n",
    "    \n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    input_length = inputs.input_ids.shape[1]\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   Input tokens: {input_length}\")\n",
    "    \n",
    "    # Generate - disable cache to avoid DynamicCache compatibility issues\n",
    "    generation_kwargs = {\n",
    "        \"max_new_tokens\": max_new_tokens,\n",
    "        \"do_sample\": (temperature > 0.0),\n",
    "        \"pad_token_id\": tokenizer.pad_token_id,\n",
    "        \"eos_token_id\": tokenizer.eos_token_id,\n",
    "        \"use_cache\": False,  # Disable cache to avoid 'seen_tokens' error\n",
    "    }\n",
    "    \n",
    "    # Only add temperature/top_p if sampling is enabled\n",
    "    if temperature > 0.0:\n",
    "        generation_kwargs[\"temperature\"] = temperature\n",
    "        generation_kwargs[\"top_p\"] = top_p\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            **generation_kwargs\n",
    "        )\n",
    "    \n",
    "    # Decode only the generated tokens (not the input prompt)\n",
    "    generated_ids = outputs[0][input_length:]\n",
    "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    output_length = len(generated_ids)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"   Generated tokens: {output_length}\")\n",
    "        print(f\"   Output length: {len(generated_text)} characters\")\n",
    "    \n",
    "    # Parse JSON with multiple strategies\n",
    "    \n",
    "    # Strategy 1: Direct JSON parse\n",
    "    try:\n",
    "        result = json.loads(generated_text)\n",
    "        if verbose:\n",
    "            print(f\"   ‚úÖ Parsed successfully (strategy 1: direct parse)\")\n",
    "        return result\n",
    "    except json.JSONDecodeError:\n",
    "        pass\n",
    "    \n",
    "    # Strategy 2: Find JSON block\n",
    "    try:\n",
    "        start = generated_text.find('{')\n",
    "        end = generated_text.rfind('}') + 1\n",
    "        if start != -1 and end > start:\n",
    "            json_text = generated_text[start:end]\n",
    "            result = json.loads(json_text)\n",
    "            if verbose:\n",
    "                print(f\"   ‚úÖ Parsed successfully (strategy 2: find JSON block)\")\n",
    "            return result\n",
    "    except (json.JSONDecodeError, ValueError):\n",
    "        pass\n",
    "    \n",
    "    # Strategy 3: Split by markers\n",
    "    for marker in [\"<|assistant|>\", \"assistant:\", \"Assistant:\"]:\n",
    "        if marker in generated_text:\n",
    "            text = generated_text.split(marker)[-1].strip()\n",
    "            try:\n",
    "                result = json.loads(text)\n",
    "                if verbose:\n",
    "                    print(f\"   ‚úÖ Parsed successfully (strategy 3: split by '{marker}')\")\n",
    "                return result\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "    \n",
    "    # Parsing failed - return error info\n",
    "    if verbose:\n",
    "        print(f\"   ‚ùå Failed to parse JSON output\")\n",
    "    \n",
    "    return {\n",
    "        \"error\": \"parsing_failed\",\n",
    "        \"output_length\": len(generated_text),\n",
    "        \"tokens_generated\": output_length,\n",
    "        \"raw_output_preview\": generated_text[:1000]\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Inference function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c49b39",
   "metadata": {},
   "source": [
    "## 5. Test the Model\n",
    "\n",
    "Let's test with various CAD instructions to see how the model performs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52ed07a",
   "metadata": {},
   "source": [
    "## üö® QUICK DIAGNOSTIC TEST (512 tokens only)\n",
    "\n",
    "Let's first verify the model works with a small output to diagnose the issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5e0d2df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî¨ DIAGNOSTIC TEST - Testing with 512 tokens and cache enabled\n",
      "================================================================================\n",
      "üìù Instruction: Create a cube 10mm by 10mm by 10mm\n",
      "\n",
      "‚è±Ô∏è  Generating with max_new_tokens=512 (cache enabled)...\n",
      "   Input tokens: 82\n",
      "   Starting generation...\n",
      "   Input tokens: 82\n",
      "   Starting generation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` model input instead.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n",
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ‚úÖ Generation completed in 162.5 seconds!\n",
      "   Generated tokens: 512\n",
      "   Output length: 1049 characters\n",
      "\n",
      "üìÑ Raw Output:\n",
      "{\n",
      "  \"parts.part_1.coordinate_system.Euler Angles[0]\": 0.0,\n",
      "  \"parts.part_1.coordinate_system.Euler Angles[1]\": 0.0,\n",
      "  \"parts.part_1.coordinate_system.Euler Angles[2]\": 0.0,\n",
      "  \"parts.part_1.coordinate_system.Translation Vector[0]\": 0.0,\n",
      "  \"parts.part_1.coordinate_system.Translation Vector[1]\": 0.0,\n",
      "  \"parts.part_1.coordinate_system.Translation Vector[2]\": 0.0,\n",
      "  \"parts.part_1.description.height\": 0.009999999999999999,\n",
      "  \"parts.part_1.description.length\": 0.09999999999999999,\n",
      "  \"parts.part_1.description.name\": \"\",\n",
      "  \"parts.part_1.description.shape\": \"\",\n",
      "  \"parts.part_1.description.width\": 0.09999999999999999,\n",
      "  \"parts.part_1.extrusion.extrude_depth_opposite_normal\": 0.0,\n",
      "  \"parts.part_1.extrusion.extrude_depth_towards_normal\": 0.009999999999999999,\n",
      "  \"parts.part_1.extrusion.operation\": \"NewBodyFeatureOperation\",\n",
      "  \"parts.part_1.extrusion.sketch_scale\": 0.09999999999999999,\n",
      "  \"parts.part_1.sketch.face_1.loop_1.arc_1.End Point[0]\": 0,\n",
      "  \"parts.part_1.sketch.face_1.loop_1.arc_1.End Point[1]\n",
      "\n",
      "‚ö†Ô∏è  Could not parse as JSON (possibly truncated)\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Quick test with SMALL output and CACHE ENABLED\n",
    "print(\"üî¨ DIAGNOSTIC TEST - Testing with 512 tokens and cache enabled\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_instruction = \"Create a cube 10mm by 10mm by 10mm\"\n",
    "\n",
    "print(f\"üìù Instruction: {test_instruction}\\n\")\n",
    "print(\"‚è±Ô∏è  Generating with max_new_tokens=512 (cache enabled)...\")\n",
    "\n",
    "# Modified generation with cache enabled\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "    {\"role\": \"user\", \"content\": test_instruction},\n",
    "]\n",
    "\n",
    "formatted_prompt = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True\n",
    ")\n",
    "\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "input_length = inputs.input_ids.shape[1]\n",
    "\n",
    "print(f\"   Input tokens: {input_length}\")\n",
    "print(f\"   Starting generation...\")\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "try:\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            do_sample=False,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            # use_cache will default to True\n",
    "        )\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    generated_ids = outputs[0][input_length:]\n",
    "    generated_text = tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "    \n",
    "    print(f\"   ‚úÖ Generation completed in {elapsed:.1f} seconds!\")\n",
    "    print(f\"   Generated tokens: {len(generated_ids)}\")\n",
    "    print(f\"   Output length: {len(generated_text)} characters\")\n",
    "    print(f\"\\nüìÑ Raw Output:\\n{generated_text[:1000]}\")\n",
    "    \n",
    "    # Try to parse\n",
    "    try:\n",
    "        result = json.loads(generated_text)\n",
    "        print(f\"\\n‚úÖ Successfully parsed JSON!\")\n",
    "        print(f\"   Total parameters: {len(result)}\")\n",
    "        print(f\"   Non-zero: {sum(1 for v in result.values() if v != 0)}\")\n",
    "    except:\n",
    "        print(f\"\\n‚ö†Ô∏è  Could not parse as JSON (possibly truncated)\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùå Error: {type(e).__name__}: {str(e)[:200]}\")\n",
    "    \n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ffd66c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TESTING PARAMETER EXTRACTION MODEL\n",
      "================================================================================\n",
      "\n",
      "üìù Running 3 test cases...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TEST 1/3\n",
      "================================================================================\n",
      "üìù Instruction: Create a cube 10mm by 20mm by 30mm centered at the origin\n",
      "--------------------------------------------------------------------------------\n",
      "‚è±Ô∏è  Generating with max_new_tokens=4096...\n",
      "   Input tokens: 86\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 462.62 MiB is free. Including non-PyTorch memory, this process has 2.12 GiB memory in use. Process 1669217 has 1.03 GiB memory in use. Process 1646205 has 18.21 GiB memory in use. Process 1723537 has 1.63 GiB memory in use. Of the allocated memory 1.59 GiB is allocated by PyTorch, and 336.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müìù Instruction: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompt\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m result = \u001b[43mextract_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4096\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     23\u001b[39m results.append({\u001b[33m\"\u001b[39m\u001b[33minstruction\u001b[39m\u001b[33m\"\u001b[39m: prompt, \u001b[33m\"\u001b[39m\u001b[33mresult\u001b[39m\u001b[33m\"\u001b[39m: result})\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 61\u001b[39m, in \u001b[36mextract_parameters\u001b[39m\u001b[34m(instruction, max_new_tokens, temperature, top_p, verbose)\u001b[39m\n\u001b[32m     58\u001b[39m     generation_kwargs[\u001b[33m\"\u001b[39m\u001b[33mtop_p\u001b[39m\u001b[33m\"\u001b[39m] = top_p\n\u001b[32m     60\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m61\u001b[39m     outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_kwargs\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;03m# Decode only the generated tokens (not the input prompt)\u001b[39;00m\n\u001b[32m     67\u001b[39m generated_ids = outputs[\u001b[32m0\u001b[39m][input_length:]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/peft/peft_model.py:1704\u001b[39m, in \u001b[36mPeftModelForCausalLM.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1702\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._enable_peft_forward_hooks(*args, **kwargs):\n\u001b[32m   1703\u001b[39m         kwargs = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.special_peft_forward_args}\n\u001b[32m-> \u001b[39m\u001b[32m1704\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1705\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1706\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m.base_model.generate(**kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/transformers/generation/utils.py:2024\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[39m\n\u001b[32m   2016\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2017\u001b[39m         input_ids=input_ids,\n\u001b[32m   2018\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2019\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2020\u001b[39m         **model_kwargs,\n\u001b[32m   2021\u001b[39m     )\n\u001b[32m   2023\u001b[39m     \u001b[38;5;66;03m# 13. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2024\u001b[39m     result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2025\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2026\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2027\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlogits_warper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_warper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2028\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2029\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2030\u001b[39m \u001b[43m        \u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m=\u001b[49m\u001b[43msynced_gpus\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2031\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstreamer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2032\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2033\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2035\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2036\u001b[39m     \u001b[38;5;66;03m# 11. prepare logits warper\u001b[39;00m\n\u001b[32m   2037\u001b[39m     prepared_logits_warper = (\n\u001b[32m   2038\u001b[39m         \u001b[38;5;28mself\u001b[39m._get_logits_warper(generation_config, device=input_ids.device)\n\u001b[32m   2039\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m generation_config.do_sample\n\u001b[32m   2040\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2041\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/transformers/generation/utils.py:2982\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, logits_warper, **model_kwargs)\u001b[39m\n\u001b[32m   2979\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   2981\u001b[39m \u001b[38;5;66;03m# forward pass to get next token\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2982\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2984\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m synced_gpus \u001b[38;5;129;01mand\u001b[39;00m this_peer_finished:\n\u001b[32m   2985\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m  \u001b[38;5;66;03m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/072cb7562cb8c4adf682a8e186aaafa49469eb5d/modeling_phi3.py:1243\u001b[39m, in \u001b[36mPhi3ForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1240\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m   1242\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1243\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1244\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1247\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1250\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1251\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1252\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1253\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1255\u001b[39m hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1256\u001b[39m logits = \u001b[38;5;28mself\u001b[39m.lm_head(hidden_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/072cb7562cb8c4adf682a8e186aaafa49469eb5d/modeling_phi3.py:1121\u001b[39m, in \u001b[36mPhi3Model.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m   1111\u001b[39m     layer_outputs = \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(\n\u001b[32m   1112\u001b[39m         decoder_layer.\u001b[34m__call__\u001b[39m,\n\u001b[32m   1113\u001b[39m         hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1118\u001b[39m         use_cache,\n\u001b[32m   1119\u001b[39m     )\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1121\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1122\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1123\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1124\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1125\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1126\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1127\u001b[39m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1128\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1130\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1132\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/072cb7562cb8c4adf682a8e186aaafa49469eb5d/modeling_phi3.py:842\u001b[39m, in \u001b[36mPhi3DecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, **kwargs)\u001b[39m\n\u001b[32m    839\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    841\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m842\u001b[39m attn_outputs, self_attn_weights, present_key_value = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    843\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    844\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    845\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    846\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    847\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    848\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    849\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    851\u001b[39m hidden_states = residual + \u001b[38;5;28mself\u001b[39m.resid_attn_dropout(attn_outputs)\n\u001b[32m    853\u001b[39m residual = hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1551\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1552\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1553\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1557\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1558\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1559\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1560\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1561\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1562\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1564\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1565\u001b[39m     result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/accelerate/hooks.py:175\u001b[39m, in \u001b[36madd_hook_to_module.<locals>.new_forward\u001b[39m\u001b[34m(module, *args, **kwargs)\u001b[39m\n\u001b[32m    173\u001b[39m         output = module._old_forward(*args, **kwargs)\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m     output = \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_old_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m module._hf_hook.post_forward(module, output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/transformers_modules/microsoft/Phi-3-mini-128k-instruct/072cb7562cb8c4adf682a8e186aaafa49469eb5d/modeling_phi3.py:362\u001b[39m, in \u001b[36mPhi3Attention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[39m\n\u001b[32m    359\u001b[39m     attn_weights = attn_weights + attention_mask\n\u001b[32m    361\u001b[39m \u001b[38;5;66;03m# upcast attention to fp32\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m362\u001b[39m attn_weights = \u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mattn_weights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m.to(value_states.dtype)\n\u001b[32m    363\u001b[39m attn_weights = nn.functional.dropout(attn_weights, p=\u001b[38;5;28mself\u001b[39m.attention_dropout, training=\u001b[38;5;28mself\u001b[39m.training)\n\u001b[32m    365\u001b[39m attn_output = torch.matmul(attn_weights, value_states)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/416Final/envfin/lib/python3.11/site-packages/torch/nn/functional.py:1890\u001b[39m, in \u001b[36msoftmax\u001b[39m\u001b[34m(input, dim, _stacklevel, dtype)\u001b[39m\n\u001b[32m   1888\u001b[39m     ret = \u001b[38;5;28minput\u001b[39m.softmax(dim)\n\u001b[32m   1889\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1890\u001b[39m     ret = \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1891\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 462.00 MiB. GPU 0 has a total capacity of 23.46 GiB of which 462.62 MiB is free. Including non-PyTorch memory, this process has 2.12 GiB memory in use. Process 1669217 has 1.03 GiB memory in use. Process 1646205 has 18.21 GiB memory in use. Process 1723537 has 1.63 GiB memory in use. Of the allocated memory 1.59 GiB is allocated by PyTorch, and 336.29 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "# Test prompts\n",
    "test_prompts = [\n",
    "    \"Create a cube 10mm by 20mm by 30mm centered at the origin\",\n",
    "    \"Design a cylinder with radius 5mm and height 15mm\",\n",
    "    \"Make a rectangular block 100mm x 50mm x 25mm\",\n",
    "]\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"TESTING PARAMETER EXTRACTION MODEL\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìù Running {len(test_prompts)} test cases...\\n\")\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"TEST {i}/{len(test_prompts)}\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"üìù Instruction: {prompt}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    result = extract_parameters(prompt, max_new_tokens=4096, verbose=True)\n",
    "    results.append({\"instruction\": prompt, \"result\": result})\n",
    "    \n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    if \"error\" in result:\n",
    "        print(f\"‚ùå Error: {result['error']}\")\n",
    "        print(f\"   Tokens generated: {result.get('tokens_generated', 0)}\")\n",
    "        print(f\"   Output length: {result.get('output_length', 0)} chars\")\n",
    "        print(f\"\\n   Raw output preview (first 500 chars):\")\n",
    "        print(f\"   {result.get('raw_output_preview', '')[:500]}\")\n",
    "    else:\n",
    "        total_params = len(result)\n",
    "        non_zero_params = sum(1 for v in result.values() if v != 0)\n",
    "        \n",
    "        print(f\"‚úÖ Successfully extracted parameters\")\n",
    "        print(f\"   Total parameters: {total_params}\")\n",
    "        print(f\"   Non-zero parameters: {non_zero_params}\")\n",
    "        print(f\"   Zero-padded parameters: {total_params - non_zero_params}\")\n",
    "        \n",
    "        # Show first 15 non-zero parameters\n",
    "        print(f\"\\n   First 15 non-zero parameters:\")\n",
    "        count = 0\n",
    "        for key, value in result.items():\n",
    "            if value != 0:\n",
    "                print(f\"      {key}: {value}\")\n",
    "                count += 1\n",
    "                if count >= 15:\n",
    "                    break\n",
    "        \n",
    "        if non_zero_params > 15:\n",
    "            print(f\"      ... and {non_zero_params - 15} more non-zero parameters\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ TESTING COMPLETE\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "599d61da",
   "metadata": {},
   "source": [
    "## 6. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904376ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\" * 80)\n",
    "print(\"RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "successful = sum(1 for r in results if \"error\" not in r[\"result\"])\n",
    "failed = len(results) - successful\n",
    "\n",
    "print(f\"\\nüìä Overall Statistics:\")\n",
    "print(f\"   Total tests: {len(results)}\")\n",
    "print(f\"   Successful: {successful}\")\n",
    "print(f\"   Failed: {failed}\")\n",
    "print(f\"   Success rate: {100 * successful / len(results):.1f}%\")\n",
    "\n",
    "if successful > 0:\n",
    "    # Stats for successful extractions\n",
    "    successful_results = [r[\"result\"] for r in results if \"error\" not in r[\"result\"]]\n",
    "    \n",
    "    avg_total = sum(len(r) for r in successful_results) / len(successful_results)\n",
    "    avg_nonzero = sum(sum(1 for v in r.values() if v != 0) for r in successful_results) / len(successful_results)\n",
    "    \n",
    "    print(f\"\\nüìà Successful Extraction Statistics:\")\n",
    "    print(f\"   Avg total parameters: {avg_total:.0f}\")\n",
    "    print(f\"   Avg non-zero parameters: {avg_nonzero:.0f}\")\n",
    "    print(f\"   Avg zero-padding: {avg_total - avg_nonzero:.0f} ({100 * (avg_total - avg_nonzero) / avg_total:.1f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1a7dbde",
   "metadata": {},
   "source": [
    "## 7. Interactive Testing\n",
    "\n",
    "Test the model with your own custom instructions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3042e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive testing\n",
    "custom_instruction = \"Create a sphere with radius 8mm\"\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"CUSTOM TEST\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìù Your instruction: {custom_instruction}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "result = extract_parameters(custom_instruction, max_new_tokens=4096, verbose=True)\n",
    "\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if \"error\" in result:\n",
    "    print(f\"‚ùå Error: {result['error']}\")\n",
    "    print(f\"\\n   Raw output preview:\\n{result.get('raw_output_preview', '')[:1000]}\")\n",
    "else:\n",
    "    print(f\"‚úÖ Extracted {len(result)} parameters\")\n",
    "    print(f\"   Non-zero: {sum(1 for v in result.values() if v != 0)}\")\n",
    "    \n",
    "    print(f\"\\n   All non-zero parameters:\")\n",
    "    for key, value in result.items():\n",
    "        if value != 0:\n",
    "            print(f\"      {key}: {value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (envfin-416Final)",
   "language": "python",
   "name": "envfin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
