{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "528ab85e",
   "metadata": {},
   "source": [
    "# üîß IMPORTANT: Kernel Selection\n",
    "\n",
    "**Before running any cells, make sure you select the correct Python kernel:**\n",
    "\n",
    "1. Click the **\"Select Kernel\"** button in the top-right corner of the notebook\n",
    "2. Choose **\"Python (envfin-416Final)\"** from the list\n",
    "   - This is your virtual environment with all dependencies installed\n",
    "3. If you don't see this option, restart VS Code and try again\n",
    "\n",
    "**Why this matters**: The notebook was crashing because it was using the wrong Python environment (`/opt/miniforge3/bin/python`) which doesn't have the required packages. Your correct environment is at `~/416Final/envfin/bin/python`.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bad6d40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import torch\n",
    "import json\n",
    "import numpy as np\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    pipeline\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from trl import SFTTrainer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c1bf7dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  bitsandbytes issue detected: ModuleNotFoundError\n",
      "   This is a known compatibility issue with PyTorch 2.5.x\n",
      "   ‚úÖ Fallback: Will use standard LoRA (no quantization)\n",
      "   üìä Impact: Training will use more GPU memory but still work effectively\n",
      "   üí° For full QLoRA: downgrade to PyTorch 2.4.x or wait for bitsandbytes update\n"
     ]
    }
   ],
   "source": [
    "# Workaround for bitsandbytes compatibility with PyTorch 2.5+\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Check if bitsandbytes can import cleanly\n",
    "use_quantization = False\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    # Test if bitsandbytes actually works (not just imports)\n",
    "    _ = bnb.nn.Linear4bit(10, 10)\n",
    "    use_quantization = True\n",
    "    print(\"‚úÖ bitsandbytes loaded successfully - QLoRA (4-bit) will be used\")\n",
    "except Exception as e:\n",
    "    use_quantization = False\n",
    "    print(f\"‚ö†Ô∏è  bitsandbytes issue detected: {type(e).__name__}\")\n",
    "    print(\"   This is a known compatibility issue with PyTorch 2.5.x\")\n",
    "    print(\"   ‚úÖ Fallback: Will use standard LoRA (no quantization)\")\n",
    "    print(\"   üìä Impact: Training will use more GPU memory but still work effectively\")\n",
    "    print(\"   üí° For full QLoRA: downgrade to PyTorch 2.4.x or wait for bitsandbytes update\")\n",
    "    \n",
    "    # Disable bitsandbytes for this session\n",
    "    sys.modules['bitsandbytes'] = None\n",
    "\n",
    "# Store flag globally for later cells\n",
    "globals()['USE_QUANTIZATION'] = use_quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "02d81aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üéØ TRAINING MODE DETECTED\n",
      "======================================================================\n",
      "‚ö†Ô∏è  Mode: Standard LoRA (No Quantization)\n",
      "   ‚Ä¢ Memory: ~15-20GB VRAM\n",
      "   ‚Ä¢ Speed: Standard\n",
      "   ‚Ä¢ Method: Full precision + LoRA adapters\n",
      "   ‚Ä¢ Reason: bitsandbytes unavailable (PyTorch 2.5.x compatibility)\n",
      "\n",
      "üìç Device: cuda\n",
      "üéÆ GPU: Quadro RTX 6000\n",
      "üíæ VRAM: 25.2GB\n",
      "\n",
      "üí° Both modes produce high-quality results!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# üìä Training Mode Summary\n",
    "print(\"=\" * 70)\n",
    "print(\"üéØ TRAINING MODE DETECTED\")\n",
    "print(\"=\" * 70)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if USE_QUANTIZATION:\n",
    "    print(\"‚úÖ Mode: QLoRA (4-bit Quantization)\")\n",
    "    print(\"   ‚Ä¢ Memory: ~7-10GB VRAM\")\n",
    "    print(\"   ‚Ä¢ Speed: Faster\")\n",
    "    print(\"   ‚Ä¢ Method: 4-bit NF4 quantization + LoRA adapters\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Mode: Standard LoRA (No Quantization)\")\n",
    "    print(\"   ‚Ä¢ Memory: ~15-20GB VRAM\")\n",
    "    print(\"   ‚Ä¢ Speed: Standard\")\n",
    "    print(\"   ‚Ä¢ Method: Full precision + LoRA adapters\")\n",
    "    print(\"   ‚Ä¢ Reason: bitsandbytes unavailable (PyTorch 2.5.x compatibility)\")\n",
    "\n",
    "print(f\"\\nüìç Device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"üéÆ GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"üíæ VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB\")\n",
    "\n",
    "print(\"\\nüí° Both modes produce high-quality results!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16837d9",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Environment Setup & Dependency Check\n",
    "\n",
    "**Automatic Mode Detection:**\n",
    "- ‚úÖ **bitsandbytes works**: Notebook will use **QLoRA** (4-bit quantization - memory efficient)\n",
    "- ‚ö†Ô∏è **bitsandbytes fails**: Notebook will use **Standard LoRA** (no quantization - more memory but still effective)\n",
    "\n",
    "**If you see the warning above:**\n",
    "This is a known compatibility issue with PyTorch 2.5.x and bitsandbytes. The notebook will automatically use standard LoRA instead.\n",
    "\n",
    "**To enable full QLoRA (optional):**\n",
    "```bash\n",
    "pip uninstall -y torch torchvision torchaudio\n",
    "pip install torch==2.4.0 torchvision==0.19.0 torchaudio==2.4.0 --index-url https://download.pytorch.org/whl/cu121\n",
    "```\n",
    "\n",
    "**Both modes work well!** The main difference is memory usage:\n",
    "- **QLoRA**: ~7-10GB VRAM for Phi-3-mini\n",
    "- **Standard LoRA**: ~15-20GB VRAM for Phi-3-mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b9ca095d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "CUDA available: True\n",
      "GPU: Quadro RTX 6000\n",
      "GPU Memory: 25.19 GB\n"
     ]
    }
   ],
   "source": [
    "# Configuration\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Use Phi-3-mini-128k for longer context (recommended)\n",
    "model_name = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "\n",
    "# Check device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02450647",
   "metadata": {},
   "source": [
    "# LoRA Fine-tuning: Phi-3-mini for CAD-to-Language Generation\n",
    "\n",
    "This notebook demonstrates fine-tuning Phi-3-mini-128k using LoRA (Low-Rank Adaptation) to create a CAD-to-Language model using the CADmium dataset.\n",
    "\n",
    "## Architecture\n",
    "- **Base Model**: microsoft/Phi-3-mini-128k-instruct (longer context for CAD designs)\n",
    "- **Training Method**: QLoRA (4-bit quantization + LoRA)\n",
    "- **Dataset**: chandar-lab/CADmium (subset for demo)\n",
    "- **Task**: Natural Language ‚Üí CAD JSON generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac1adae7",
   "metadata": {},
   "source": [
    "## 1. Install Required Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "aa3b9c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary packages\n",
    "# !pip install -q transformers datasets peft accelerate bitsandbytes trl sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0571a1",
   "metadata": {},
   "source": [
    "## 2. Load and Prepare CADmium Dataset\n",
    "\n",
    "We'll load a subset of the CADmium dataset and prepare it for fine-tuning with proper formatting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e42bf1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CADmium dataset...\n",
      "‚úÖ Loaded 100 samples from CADmium\n",
      "Dataset columns: ['uid', 'annotation', 'json_desc']\n",
      "\n",
      "First example:\n",
      "{'uid': '0072/00726842', 'annotation': 'Begin by creating a rectangular prism with overall dimensions 0.75 long, 0.375 wide, and 0.46875 high. \\n\\nNext, modify the ends of the prism as follows:\\n\\nAt each of the four vertical corners (both at x=0 and x=0.75 along the length), replace the sharp edge with a quarter-circle arc of radius 0.1875, centered horizontally and vertically on the face. The top and bottom vertical edges on the short faces (width sides) are thus rounded, blending tangent to both edge and face.\\n\\nOn the top and bottom faces, ensure each end describes a smooth semicircular extension: extrude the width at both ends (x=0 and x=0.75) into a half-cylinder, each with a radius 0.1875 and center at (0.1875, 0.1875) for x=0 and (0.5625, 0.1875) for x=0.75. The total length from tip to tip, including both half-cylindrical ends, is 0.75.\\n\\nBlend all transitions smoothly so the entire object forms a rectangular solid with full width, but with its ends fully rounded in both plan and elevation, producing a pill-shaped bar with rectangular cross-section and semicircular ends on both main faces and edges. All features should be symmetrical about the object‚Äôs longitudinal axis.', 'json_desc': '{\"parts\": {\"part_1\": {\"coordinate_system\": {\"Euler Angles\": [0.0, 0.0, 0.0], \"Translation Vector\": [0.0, 0.0, 0.0]}, \"sketch\": {\"face_1\": {\"loop_1\": {\"arc_1\": {\"Start Point\": [0.0, 0.1875], \"Mid Point\": [0.0549, 0.3201], \"End Point\": [0.1875, 0.375]}, \"line_1\": {\"Start Point\": [0.1875, 0.375], \"End Point\": [0.0, 0.375]}, \"line_2\": {\"Start Point\": [0.0, 0.375], \"End Point\": [0.0, 0.1875]}}}, \"face_2\": {\"loop_1\": {\"line_1\": {\"Start Point\": [0.1875, 0.0], \"End Point\": [0.375, 0.0]}, \"line_2\": {\"Start Point\": [0.375, 0.0], \"End Point\": [0.375, 0.1875]}, \"arc_1\": {\"Start Point\": [0.375, 0.1875], \"Mid Point\": [0.3201, 0.0549], \"End Point\": [0.1875, 0.0]}}}, \"face_3\": {\"loop_1\": {\"arc_1\": {\"Start Point\": [0.375, 0.1875], \"Mid Point\": [0.4299, 0.3201], \"End Point\": [0.5625, 0.375]}, \"line_1\": {\"Start Point\": [0.5625, 0.375], \"End Point\": [0.375, 0.375]}, \"line_2\": {\"Start Point\": [0.375, 0.375], \"End Point\": [0.375, 0.1875]}}}, \"face_4\": {\"loop_1\": {\"line_1\": {\"Start Point\": [0.5625, 0.0], \"End Point\": [0.75, 0.0]}, \"line_2\": {\"Start Point\": [0.75, 0.0], \"End Point\": [0.75, 0.1875]}, \"arc_1\": {\"Start Point\": [0.75, 0.1875], \"Mid Point\": [0.6951, 0.0549], \"End Point\": [0.5625, 0.0]}}}}, \"extrusion\": {\"extrude_depth_towards_normal\": 0.4687, \"extrude_depth_opposite_normal\": 0.0, \"sketch_scale\": 0.75, \"operation\": \"NewBodyFeatureOperation\"}, \"description\": {\"name\": \"\", \"shape\": \"\", \"length\": 0.75, \"width\": 0.37499999999999994, \"height\": 0.46874999999999994}}, \"part_2\": {\"coordinate_system\": {\"Euler Angles\": [0.0, 0.0, 0.0], \"Translation Vector\": [0.0, 0.0, 0.0]}, \"sketch\": {\"face_1\": {\"loop_1\": {\"line_1\": {\"Start Point\": [0.0, 0.0], \"End Point\": [0.1875, 0.0]}, \"arc_1\": {\"Start Point\": [0.1875, 0.0], \"Mid Point\": [0.0549, 0.0549], \"End Point\": [0.0, 0.1875]}, \"line_2\": {\"Start Point\": [0.0, 0.1875], \"End Point\": [0.0, 0.0]}}}, \"face_2\": {\"loop_1\": {\"arc_1\": {\"Start Point\": [0.1875, 0.375], \"Mid Point\": [0.3201, 0.3201], \"End Point\": [0.375, 0.1875]}, \"line_1\": {\"Start Point\": [0.375, 0.1875], \"End Point\": [0.375, 0.375]}, \"line_2\": {\"Start Point\": [0.375, 0.375], \"End Point\": [0.1875, 0.375]}}}, \"face_3\": {\"loop_1\": {\"line_1\": {\"Start Point\": [0.375, 0.0], \"End Point\": [0.5625, 0.0]}, \"arc_1\": {\"Start Point\": [0.5625, 0.0], \"Mid Point\": [0.4299, 0.0549], \"End Point\": [0.375, 0.1875]}, \"line_2\": {\"Start Point\": [0.375, 0.1875], \"End Point\": [0.375, 0.0]}}}, \"face_4\": {\"loop_1\": {\"arc_1\": {\"Start Point\": [0.5625, 0.375], \"Mid Point\": [0.6951, 0.3201], \"End Point\": [0.75, 0.1875]}, \"line_1\": {\"Start Point\": [0.75, 0.1875], \"End Point\": [0.75, 0.375]}, \"line_2\": {\"Start Point\": [0.75, 0.375], \"End Point\": [0.5625, 0.375]}}}}, \"extrusion\": {\"extrude_depth_towards_normal\": 0.4687, \"extrude_depth_opposite_normal\": 0.0, \"sketch_scale\": 0.75, \"operation\": \"NewBodyFeatureOperation\"}, \"description\": {\"name\": \"\", \"shape\": \"\", \"length\": 0.75, \"width\": 0.37499999999999994, \"height\": 0.46874999999999994}}, \"part_3\": {\"coordinate_system\": {\"Euler Angles\": [0.0, 0.0, 0.0], \"Translation Vector\": [0.0, 0.0, 0.0]}, \"sketch\": {\"face_1\": {\"loop_1\": {\"circle_1\": {\"Center\": [0.1875, 0.1875], \"Radius\": 0.1875}}}, \"face_2\": {\"loop_1\": {\"circle_1\": {\"Center\": [0.5625, 0.1875], \"Radius\": 0.1875}}}}, \"extrusion\": {\"extrude_depth_towards_normal\": 0.4687, \"extrude_depth_opposite_normal\": 0.0, \"sketch_scale\": 0.75, \"operation\": \"NewBodyFeatureOperation\"}, \"description\": {\"name\": \"\", \"shape\": \"\", \"length\": 0.7499999999999999, \"width\": 0.37499999999999994, \"height\": 0.46874999999999994}}}}'}\n"
     ]
    }
   ],
   "source": [
    "# Load CADmium dataset (subset for demo to save memory)\n",
    "print(\"Loading CADmium dataset...\")\n",
    "try:\n",
    "    # Load from HuggingFace - we'll use a small subset\n",
    "    dataset = load_dataset(\"chandar-lab/CADmium-ds\", split=\"train\", streaming=False)\n",
    "    \n",
    "    # Take a small subset for this demo (adjust based on your memory)\n",
    "    num_samples = 100  # Start with 100 samples for demo\n",
    "    dataset = dataset.shuffle(seed=42).select(range(min(num_samples, len(dataset))))\n",
    "    \n",
    "    print(f\"‚úÖ Loaded {len(dataset)} samples from CADmium\")\n",
    "    print(f\"Dataset columns: {dataset.column_names}\")\n",
    "    print(f\"\\nFirst example:\")\n",
    "    print(dataset[0])\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading dataset: {e}\")\n",
    "    print(\"\\nWill create a synthetic example dataset for demonstration...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0feb4b96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26c6129b6575469a8cc7d75a8a9081a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Formatting CAD instructions:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Formatted 100 examples\n",
      "\n",
      "Sample formatted example:\n",
      "System: You map natural language instructions to a corresponding Fusion 360 JSON using t...\n",
      "User: Begin by creating a rectangular prism with overall dimensions 0.75 long, 0.375 w...\n",
      "Assistant: {}...\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing function\n",
    "def format_cad_instruction(example):\n",
    "    \"\"\"\n",
    "    Format the CADmium-ds dataset into instruction-following format.\n",
    "    \n",
    "    CADmium-ds structure:\n",
    "    - name: Design name\n",
    "    - annotation: Natural language description\n",
    "    - sequence: CAD operations (JSON/dict)\n",
    "    \"\"\"\n",
    "    # Extract instruction and JSON from CADmium-ds format\n",
    "    instruction = example.get('annotation', 'Create a CAD model')\n",
    "    \n",
    "    # Handle sequence - could be dict or string\n",
    "    sequence = example.get('sequence', {})\n",
    "    if isinstance(sequence, dict):\n",
    "        json_output = json.dumps(sequence, indent=2)\n",
    "    elif isinstance(sequence, str):\n",
    "        try:\n",
    "            # Parse and re-format JSON to ensure consistency\n",
    "            json_obj = json.loads(sequence)\n",
    "            json_output = json.dumps(json_obj, indent=2)\n",
    "        except:\n",
    "            json_output = sequence  # Keep as is if not valid JSON\n",
    "    else:\n",
    "        json_output = '{}'\n",
    "    \n",
    "    # Format according to Phi-3 chat template\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You map natural language instructions to a corresponding Fusion 360 JSON using the v1.0 schema. Generate valid, executable CAD JSON with proper units (meters), coordinate frames, and operation ordering (sketch ‚Üí feature ‚Üí transform).\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": instruction\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": json_output\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    return {\"messages\": messages}\n",
    "\n",
    "# Apply formatting\n",
    "print(\"Formatting dataset...\")\n",
    "try:\n",
    "    formatted_dataset = dataset.map(\n",
    "        format_cad_instruction, \n",
    "        remove_columns=dataset.column_names,\n",
    "        desc=\"Formatting CAD instructions\"\n",
    "    )\n",
    "    print(f\"‚úÖ Formatted {len(formatted_dataset)} examples\")\n",
    "    print(f\"\\nSample formatted example:\")\n",
    "    print(f\"System: {formatted_dataset[0]['messages'][0]['content'][:80]}...\")\n",
    "    print(f\"User: {formatted_dataset[0]['messages'][1]['content'][:80]}...\")\n",
    "    print(f\"Assistant: {formatted_dataset[0]['messages'][2]['content'][:150]}...\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during formatting: {e}\")\n",
    "    # This shouldn't happen with synthetic data, but just in case\n",
    "    print(\"\\nUsing pre-formatted synthetic dataset...\")\n",
    "    synthetic_data = [\n",
    "        {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You map natural language instructions to a corresponding Fusion 360 JSON using the v1.0 schema.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Create a rectangular sketch 10mm by 20mm centered at the origin\"},\n",
    "                {\"role\": \"assistant\", \"content\": '{\"parts\": {\"part_0\": {\"sketch\": {\"center\": [0, 0], \"width\": 0.01, \"height\": 0.02}, \"frame\": \"world\"}}}'}\n",
    "            ]\n",
    "        }\n",
    "    ] * 10  # Repeat for demo\n",
    "    formatted_dataset = Dataset.from_list(synthetic_data)\n",
    "    print(f\"Created synthetic dataset with {len(formatted_dataset)} examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f24e630",
   "metadata": {},
   "source": [
    "## 3. Configure QLoRA (4-bit Quantization + LoRA)\n",
    "\n",
    "QLoRA allows us to fine-tune large models efficiently by:\n",
    "- Loading the base model in 4-bit precision (nf4)\n",
    "- Adding trainable LoRA adapters to attention and MLP layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "b09a25bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ÑπÔ∏è  Standard LoRA: No quantization (bitsandbytes unavailable)\n"
     ]
    }
   ],
   "source": [
    "# Configure 4-bit quantization (if bitsandbytes is available)\n",
    "if USE_QUANTIZATION:\n",
    "    # Determine best compute dtype for quantization\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            # Test bfloat16 support on GPU\n",
    "            _ = torch.zeros(1, dtype=torch.bfloat16, device='cuda')\n",
    "            quant_compute_dtype = torch.bfloat16\n",
    "            print(\"‚úÖ Quantization will use bfloat16\")\n",
    "        except:\n",
    "            quant_compute_dtype = torch.float16\n",
    "            print(\"‚ö†Ô∏è  Quantization will use float16 (bfloat16 not supported)\")\n",
    "    else:\n",
    "        quant_compute_dtype = torch.float32\n",
    "        print(\"‚ÑπÔ∏è  CPU mode: quantization will use float32\")\n",
    "\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",  # Normal Float 4-bit\n",
    "        bnb_4bit_compute_dtype=quant_compute_dtype,\n",
    "        bnb_4bit_use_double_quant=True,  # Double quantization for even more memory savings\n",
    "    )\n",
    "    print(\"‚úÖ QLoRA: 4-bit quantization config created\")\n",
    "else:\n",
    "    bnb_config = None\n",
    "    print(\"‚ÑπÔ∏è  Standard LoRA: No quantization (bitsandbytes unavailable)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d4ee18a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LoRA config created\n",
      "   Rank: 16\n",
      "   Alpha: 16\n",
      "   Dropout: 0.05\n",
      "   Target modules: {'o_proj', 'down_proj', 'k_proj', 'q_proj', 'up_proj', 'v_proj', 'gate_proj'}\n"
     ]
    }
   ],
   "source": [
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=16,  # Rank - controls adapter capacity (16-32 recommended)\n",
    "    lora_alpha=16,  # Scaling factor (usually equal to r)\n",
    "    lora_dropout=0.05,  # Dropout for regularization\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    # Target all attention and MLP modules for comprehensive adaptation\n",
    "    target_modules=[\n",
    "        \"q_proj\",    # Query projection\n",
    "        \"k_proj\",    # Key projection\n",
    "        \"v_proj\",    # Value projection\n",
    "        \"o_proj\",    # Output projection\n",
    "        \"gate_proj\", # MLP gate\n",
    "        \"up_proj\",   # MLP up projection\n",
    "        \"down_proj\"  # MLP down projection\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LoRA config created\")\n",
    "print(f\"   Rank: {lora_config.r}\")\n",
    "print(f\"   Alpha: {lora_config.lora_alpha}\")\n",
    "print(f\"   Dropout: {lora_config.lora_dropout}\")\n",
    "print(f\"   Target modules: {lora_config.target_modules}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4bc6bf1",
   "metadata": {},
   "source": [
    "## 4. Load Base Model and Tokenizer with QLoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "76c13fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer from microsoft/Phi-3-mini-128k-instruct...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Tokenizer loaded\n",
      "   Vocab size: 32011\n",
      "   Pad token: <|endoftext|>\n",
      "   EOS token: <|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "print(f\"Loading tokenizer from {model_name}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    padding_side=\"right\",  # Required for training\n",
    ")\n",
    "\n",
    "# Set padding token\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "print(f\"‚úÖ Tokenizer loaded\")\n",
    "print(f\"   Vocab size: {len(tokenizer)}\")\n",
    "print(f\"   Pad token: {tokenizer.pad_token}\")\n",
    "print(f\"   EOS token: {tokenizer.eos_token}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "85f0bd68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model microsoft/Phi-3-mini-128k-instruct in standard precision (no quantization)...\n",
      "‚è≥ This may take a few minutes...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b283fe2dd69d4cf4930d17d0ec05807b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded in torch.bfloat16 (standard LoRA, no quantization)\n",
      "\n",
      "‚úÖ LoRA adapters added\n",
      "   Trainable params: 8,912,896 (0.23%)\n",
      "   Total params: 3,829,992,448\n"
     ]
    }
   ],
   "source": [
    "# Load model (with or without quantization)\n",
    "if USE_QUANTIZATION:\n",
    "    print(f\"Loading model {model_name} with 4-bit quantization (QLoRA)...\")\n",
    "    print(\"‚è≥ This may take a few minutes...\")\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=torch.bfloat16,\n",
    "        attn_implementation=\"eager\",\n",
    "    )\n",
    "    \n",
    "    # Prepare model for k-bit training\n",
    "    model = prepare_model_for_kbit_training(model)\n",
    "    # Add LoRA adapters\n",
    "else:\n",
    "    print(f\"Loading model {model_name} in standard precision (no quantization)...\")\n",
    "    print(\"‚è≥ This may take a few minutes...\")\n",
    "    \n",
    "    # Determine best dtype\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            _ = torch.zeros(1, dtype=torch.bfloat16, device='cuda')\n",
    "            model_dtype = torch.bfloat16\n",
    "        except:\n",
    "            model_dtype = torch.float16\n",
    "    else:\n",
    "        model_dtype = torch.float32\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True,\n",
    "        torch_dtype=model_dtype,\n",
    "        attn_implementation=\"eager\",\n",
    "    )\n",
    "    print(f\"‚úÖ Model loaded in {model_dtype} (standard LoRA, no quantization)\")\n",
    "\n",
    "# Prepare model for k-bit training (gradient checkpointing, etc.)\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# Print trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\n‚úÖ LoRA adapters added\")\n",
    "print(f\"   Trainable params: {trainable_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "print(f\"   Total params: {total_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13941392",
   "metadata": {},
   "source": [
    "## 5. Configure Training with SFT (Supervised Fine-Tuning)\n",
    "\n",
    "Following the recommendations:\n",
    "- Learning rate: 2e-4 with cosine schedule\n",
    "- Warmup: 3%\n",
    "- Sequence length: 2-4k tokens\n",
    "- Effective batch size: 256-512 tokens/step\n",
    "- Training: 2-3 epochs with early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "56099795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training configuration created\n",
      "   Epochs: 2\n",
      "   Batch size: 1\n",
      "   Gradient accumulation: 8\n",
      "   Effective batch size: 8\n",
      "   Learning rate: 0.0002\n",
      "   LR scheduler: SchedulerType.COSINE\n"
     ]
    }
   ],
   "source": [
    "# Training configuration\n",
    "output_dir = \"./phi3-cad-lora\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=2,  # 2-3 epochs recommended\n",
    "    per_device_train_batch_size=1,  # Small batch size for memory efficiency\n",
    "    gradient_accumulation_steps=8,  # Effective batch size = 8\n",
    "    learning_rate=2e-4,  # Recommended for LoRA\n",
    "    lr_scheduler_type=\"cosine\",  # Cosine learning rate schedule\n",
    "    warmup_ratio=0.03,  # 3% warmup\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    fp16=False,  # Use bfloat16 instead\n",
    "    bf16=True,  # Better for training stability\n",
    "    gradient_checkpointing=True,  # Save memory\n",
    "    optim=\"paged_adamw_8bit\" if USE_QUANTIZATION else \"adamw_torch\",  # 8-bit optimizer if quantized\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard for demo\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training configuration created\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   LR scheduler: {training_args.lr_scheduler_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1a92f107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training data formatting...\n",
      "‚úÖ Trainer initialized\n",
      "   Training samples: 100\n"
     ]
    }
   ],
   "source": [
    "# Format messages to text using chat template\n",
    "def formatting_prompts_func(examples):\n",
    "    \"\"\"\n",
    "    Format examples for SFTTrainer.\n",
    "    Must return a list of strings (one per example).\n",
    "    \"\"\"\n",
    "    texts = []\n",
    "    for messages in examples[\"messages\"]:\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=False\n",
    "        )\n",
    "        texts.append(text)\n",
    "    return texts\n",
    "\n",
    "print(\"Setting up training data formatting...\")\n",
    "\n",
    "# Initialize SFT Trainer (compatible with older TRL versions)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=formatted_dataset,  # Use the dataset with \"messages\" field\n",
    "    args=training_args,\n",
    "    formatting_func=formatting_prompts_func,  # Function to convert messages to text\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Trainer initialized\")\n",
    "print(f\"   Training samples: {len(formatted_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b58b83d",
   "metadata": {},
   "source": [
    "## 6. Train the Model\n",
    "\n",
    "Start the LoRA fine-tuning process. This will only train the LoRA adapter weights (~0.5-2% of total parameters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "502432fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting training...\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='26' max='26' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [26/26 05:33, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.169600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.340900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.118100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2.398300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.083300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.776800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.841300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.922600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>1.938600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.670300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>1.883400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>1.754300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>1.549400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>1.647700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.635900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>1.528300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>1.557800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>1.590100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>1.546600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.492900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>1.632000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>1.362900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>1.442200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>1.388300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>1.410700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>1.384800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "‚úÖ Training completed!\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "print(\"üöÄ Starting training...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"‚úÖ Training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f01414",
   "metadata": {},
   "source": [
    "## 7. Save the LoRA Adapters\n",
    "\n",
    "Save only the trained LoRA adapters (much smaller than the full model)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "39defbde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LoRA adapters saved to: ./phi3-cad-lora-adapters\n",
      "\n",
      "You can load these adapters later with:\n",
      "  from peft import PeftModel\n",
      "  base_model = AutoModelForCausalLM.from_pretrained('microsoft/Phi-3-mini-128k-instruct')\n",
      "  model = PeftModel.from_pretrained(base_model, './phi3-cad-lora-adapters')\n"
     ]
    }
   ],
   "source": [
    "# Save LoRA adapters\n",
    "lora_output_dir = \"./phi3-cad-lora-adapters\"\n",
    "\n",
    "model.save_pretrained(lora_output_dir)\n",
    "tokenizer.save_pretrained(lora_output_dir)\n",
    "\n",
    "print(f\"‚úÖ LoRA adapters saved to: {lora_output_dir}\")\n",
    "print(\"\\nYou can load these adapters later with:\")\n",
    "print(f\"  from peft import PeftModel\")\n",
    "print(f\"  base_model = AutoModelForCausalLM.from_pretrained('{model_name}')\")\n",
    "print(f\"  model = PeftModel.from_pretrained(base_model, '{lora_output_dir}')\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e4abc32",
   "metadata": {},
   "source": [
    "## 8. Test the Fine-tuned Model\n",
    "\n",
    "Generate CAD JSON from natural language instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "270fd473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Generation function ready (cache disabled for compatibility)\n"
     ]
    }
   ],
   "source": [
    "# Test generation function (alternative - disable cache)\n",
    "def generate_cad_json(instruction, max_new_tokens=512, temperature=0.7, top_p=0.9):\n",
    "    \"\"\"Generate CAD JSON from natural language instruction.\"\"\"\n",
    "    \n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You map natural language instructions to a corresponding Fusion 360 JSON using the v1.0 schema. Generate valid, executable CAD JSON with proper units (meters), coordinate frames, and operation ordering (sketch ‚Üí feature ‚Üí transform).\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": instruction\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Format using chat template\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    \n",
    "    # Tokenize\n",
    "    inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    # Generate WITHOUT cache (slower but avoids compatibility issues)\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id,\n",
    "            use_cache=False,  # Disable cache to avoid DynamicCache error\n",
    "        )\n",
    "    \n",
    "    # Decode\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the assistant's response\n",
    "    if \"<|assistant|>\" in generated_text:\n",
    "        generated_text = generated_text.split(\"<|assistant|>\")[-1].strip()\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "print(\"‚úÖ Generation function ready (cache disabled for compatibility)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "28ab792b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing fine-tuned model\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìù Test 1: Create a rectangular sketch 10mm by 20mm centered at the origin\n",
      "--------------------------------------------------------------------------------\n",
      "Generated CAD JSON:\n",
      "You map natural language instructions to a corresponding Fusion 360 JSON using the v1.0 schema. Generate valid, executable CAD JSON with proper units (meters), coordinate frames, and operation ordering (sketch ‚Üí feature ‚Üí transform). Create a rectangular sketch 10mm by 20mm centered at the origin {\n",
      "  \"$schema\": \"https://www.autodesk.com/specification/fusion360\",\n",
      "  \"name\": \"Sketch\",\n",
      "  \"geometry\": {\n",
      "    \"type\": \"Sketch\",\n",
      "    \"origin\": {\n",
      "      \"x\": 0,\n",
      "      \"y\": 0,\n",
      "      \"z\": 0\n",
      "    },\n",
      "    \"planes\": [\n",
      "      {\n",
      "        \"name\": \"XY\",\n",
      "        \"points\": [\n",
      "          {\n",
      "            \"x\": 0,\n",
      "            \"y\": 0,\n",
      "            \"z\": 0\n",
      "          },\n",
      "          {\n",
      "            \"x\": 5,\n",
      "            \"y\": 0,\n",
      "            \"z\": 0\n",
      "          },\n",
      "          {\n",
      "            \"x\": 5,\n",
      "            \"y\": -5,\n",
      "            \"z\": 0\n",
      "          },\n",
      "          {\n",
      "            \"x\": 0,\n",
      "            \"y\": -5,\n",
      "            \"z\": 0\n",
      "          }\n",
      "        ]\n",
      "      },\n",
      "      {\n",
      "        \"name\": \"XZ\",\n",
      "        \"points\": [\n",
      "          {\n",
      "            \"x\": 0\n",
      "================================================================================\n",
      "\n",
      "üìù Test 2: Make a circular sketch with radius 5mm at position (10, 10)\n",
      "--------------------------------------------------------------------------------\n",
      "Generated CAD JSON:\n",
      "You map natural language instructions to a corresponding Fusion 360 JSON using the v1.0 schema. Generate valid, executable CAD JSON with proper units (meters), coordinate frames, and operation ordering (sketch ‚Üí feature ‚Üí transform). Make a circular sketch with radius 5mm at position (10, 10) {\n",
      "  \"units\": \"millimeters\",\n",
      "  \"origin\": {\n",
      "    \"x\": 0,\n",
      "    \"y\": 0,\n",
      "    \"z\": 0\n",
      "  },\n",
      "  \"transform\": {\n",
      "    \"center\": {\n",
      "      \"x\": 10,\n",
      "      \"y\": 10,\n",
      "      \"z\": 0\n",
      "    },\n",
      "    \"rotation\": {\n",
      "      \"type\": \"axis\",\n",
      "      \"angle\": 0,\n",
      "      \"direction\": {\n",
      "        \"x\": 0,\n",
      "        \"y\": 0,\n",
      "        \"z\": 1\n",
      "      }\n",
      "    },\n",
      "    \"scale\": {\n",
      "      \"x\": 1,\n",
      "      \"y\": 1,\n",
      "      \"z\": 1\n",
      "    }\n",
      "  },\n",
      "  \"sketchPlane\": {\n",
      "    \"normal\": {\n",
      "      \"x\": 0,\n",
      "      \"y\": 0,\n",
      "      \"z\": 1\n",
      "    },\n",
      "    \"point\": {\n",
      "      \"x\": 10,\n",
      "      \"y\": 10,\n",
      "      \"z\": 0\n",
      "    }\n",
      "  },\n",
      "\n",
      "================================================================================\n",
      "\n",
      "üìù Test 3: Create a cube with side length 10 units\n",
      "--------------------------------------------------------------------------------\n",
      "Generated CAD JSON:\n",
      "You map natural language instructions to a corresponding Fusion 360 JSON using the v1.0 schema. Generate valid, executable CAD JSON with proper units (meters), coordinate frames, and operation ordering (sketch ‚Üí feature ‚Üí transform). Create a cube with side length 10 units {\n",
      "  \"$schema\": \"https://model.fusion3d.com/v1/json/model.json\",\n",
      "  \"version\": \"1.0.0\",\n",
      "  \"geometry\": {\n",
      "    \"solid\": {\n",
      "      \"path\": \"//Solid//Wire//Vertex//Vertex\",\n",
      "      \"material\": \"Material//Reflectance\"\n",
      "    }\n",
      "  },\n",
      "  \"parameters\": {\n",
      "    \"Cube:Extents\": {\n",
      "      \"name\": \"Extents\",\n",
      "      \"type\": \"Array[Float]\",\n",
      "      \"minimum\": [0],\n",
      "      \"maximum\": [10],\n",
      "      \"default\": [10, 10, 10]\n",
      "    }\n",
      "  },\n",
      "  \"transform\": {\n",
      "    \"matrix\": [\n",
      "      1.0, 0.0, 0.0, 0.0,\n",
      "      0.0, 1.0, 0.0, 0.0,\n",
      "      0.0, 0.0, 1.0, 0.0,\n",
      "      0.0, 0.0, 0.0,\n",
      "================================================================================\n",
      "\n",
      "üìù Test 4: Design a cylinder with radius 3mm and height 15mm\n",
      "--------------------------------------------------------------------------------\n",
      "Generated CAD JSON:\n",
      "You map natural language instructions to a corresponding Fusion 360 JSON using the v1.0 schema. Generate valid, executable CAD JSON with proper units (meters), coordinate frames, and operation ordering (sketch ‚Üí feature ‚Üí transform). Design a cylinder with radius 3mm and height 15mm {\n",
      "  \"steps\": [\n",
      "    {\n",
      "      \"type\": \"sketch\",\n",
      "      \"params\": {\n",
      "        \"origin\": [0, 0, 0],\n",
      "        \"x_axis\": [1, 0, 0],\n",
      "        \"y_axis\": [0, 1, 0]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"plane\",\n",
      "      \"params\": {\n",
      "        \"origin\": [0, 0, 0],\n",
      "        \"x_axis\": [1, 0, 0],\n",
      "        \"y_axis\": [0, 1, 0]\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"circle\",\n",
      "      \"params\": {\n",
      "        \"center\": [0, 0, 0],\n",
      "        \"radius\": 0.003\n",
      "      }\n",
      "    },\n",
      "    {\n",
      "      \"type\": \"extrude\",\n",
      "      \"params\": {\n",
      "        \"depth\": 0.015\n",
      "      }\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Test with sample prompts\n",
    "test_prompts = [\n",
    "    \"Create a rectangular sketch 10mm by 20mm centered at the origin\",\n",
    "    \"Make a circular sketch with radius 5mm at position (10, 10)\",\n",
    "    \"Create a cube with side length 10 units\",\n",
    "    \"Design a cylinder with radius 3mm and height 15mm\"\n",
    "]\n",
    "\n",
    "print(\"üß™ Testing fine-tuned model\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for i, prompt in enumerate(test_prompts, 1):\n",
    "    print(f\"\\nüìù Test {i}: {prompt}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    result = generate_cad_json(prompt, max_new_tokens=256)\n",
    "    print(f\"Generated CAD JSON:\\n{result}\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bb839b0",
   "metadata": {},
   "source": [
    "## 9. Advanced: Load and Swap LoRA Adapters\n",
    "\n",
    "Demonstration of hot-swapping adapters (for future dual-adapter setup: NL‚ÜíCAD and CAD‚ÜíNL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "871582d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: How to load saved LoRA adapters later\n",
    "\"\"\"\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model (4-bit quantized)\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-128k-instruct\",\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "# Load LoRA adapter for NL‚ÜíCAD generation\n",
    "model_nl_to_cad = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"./phi3-cad-lora-adapters\"\n",
    ")\n",
    "\n",
    "# For a dual-adapter setup, you could train a second adapter:\n",
    "# model_cad_to_nl = PeftModel.from_pretrained(\n",
    "#     base_model,\n",
    "#     \"./phi3-cad-to-nl-lora-adapters\"\n",
    "# )\n",
    "\n",
    "# Hot-swap adapters at inference time:\n",
    "# model.set_adapter(\"nl_to_cad\")  # Switch to generation\n",
    "# model.set_adapter(\"cad_to_nl\")  # Switch to explanation\n",
    "\"\"\"\n",
    "\n",
    "print(\"‚úÖ See code comments for adapter loading/swapping example\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8fdc0f",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (envfin-416Final)",
   "language": "python",
   "name": "envfin"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
